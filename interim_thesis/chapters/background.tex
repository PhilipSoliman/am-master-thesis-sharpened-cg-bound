\chapter{Mathematical background}\label{ch:background}\newpage
In this chapter we focus on a simple Darcy problem like the one posed in \cite{acms_coarse_space_Heinlein2018,ams_coarse_space_comp_study_Alves2024}. This problem is of the form
\begin{equation}
  \label{eq:elliptic_problem}
  \begin{aligned}
    -\nabla\cdot\left(\mathcal{C}\nabla u\right) & = f \quad \text{in } \Omega,           \\
    u                                       & = u_D \quad \text{on } \partial\Omega,
  \end{aligned}
\end{equation}
where $\Omega\subset\mathbb{R}^d$ is a bounded domain with Lipschitz boundary $\partial\Omega$, $\mathcal{C}\in L^\infty(\Omega)$ is a positive coefficient, $f\in L^2(\Omega)$ is a source term, and $u\in H^1_0(\Omega)$ is the solution. The coefficient $\mathcal{C}$ is assumed to be bounded from above and below by positive constants, i.e., $0 < \mathcal{C}_{\min} \leq \mathcal{C}(x) \leq \mathcal{C}_{\max} < \infty$ for all $x\in\Omega$. The solution $u$ is assumed to be sufficiently smooth, i.e., $u\in H^2(\Omega)\cap H^1_0(\Omega)$, so that the problem is well-posed. The goal is to compute an approximation $u_h\in V_h$ to the solution $u$ using a finite-dimensional subspace $V_h\subset H^1_0(\Omega)$, where $V_h$ is spanned by a set of basis functions $\{\varphi_i\}_{i=1}^n$. The Galerkin method seeks $u_h\in V_h$ such that
\begin{equation}
  \label{eq:galerkin}
  \int_\Omega \mathcal{C}\nabla u_h\cdot\nabla v_h\,dx = \int_\Omega f v_h\,dx \quad \text{for all } v_h\in V_h.
\end{equation}
The Galerkin method leads to a linear system of equations $A \mathbf{u} = \mathbf{b}$, where $A$ is the stiffness matrix and $\mathbf{b}$ is the load vector. The stiffness matrix $A$ is symmetric and positive definite, and the load vector $\mathbf{b}$ is determined by the source term $f$ and the boundary conditions. The solution $\mathbf{u}$ can be computed using iterative methods like the conjugate gradient method, which is guaranteed to converge in a finite number of iterations for symmetric positive definite matrices in infinite precision arithmetic.

\section{Conjugate Gradient Method}\label{sec:cg_method}
The CG method is a special instance of the class of Krylov subspace methods. It is derived from the Direct Lanczos (D-Lanczos) algorithm applied to the residual of linear systems \cite[Algorithm 6.17]{iter_method_saad}. The D-Lanczos algorithm generates a sequence of orthonormal Lanczos vectors $\{\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_m\}$ that span the Krylov subspace $\mathcal{K}_m(A, \mathbf{r}_0)$, where $\mathbf{r}_0 = \mathbf{b} - A\mathbf{u}_0$ is the initial residual such that
\begin{equation}
  \mathcal{K}_m(A_0, \mathbf{r}_0) = \text{span}\{\mathbf{r}_0, A\mathbf{r}_0, A^2\mathbf{r}_0, \dots, A^{m-1}\mathbf{r}_0\},
  \label{eq:cg_krylov_space}
\end{equation}
or $\mathcal{K}_m$ as a shorthand. The approximate answer is then given by
\begin{equation}
  \mathbf{u}_m = \mathbf{u}_0 + \sum_{i=0}^{m-1} c_i A^i \mathbf{r}_0 = \mathbf{u}_0 + q_{m-1}(A)\mathbf{r}_0,
  \label{eq:cg_approximate_solution}
\end{equation}
where $q_{m-1}(A)$ is a polynomial of degree $m-1$ in $A$. It is shown later in this section how the coefficients $c_i$ are obtained (\cref{eq:cg_solution_coefficients}).

\subsection{Variants of the CG method} \label{sec:cg_variants}
Variants of the CG method differ in the way A is preconditioned (see section \cref{sec:cg_preconditioning}) d the choices for the constraint subspace $\mathcal{L}_m$. The former type of variations result in the preconditioned CG method PCG and these are described in \cref{sec:cg_preconditioning}. The latter type of variations branch off into two major categories:
\begin{enumerate}[label=\roman*,ref=CG-type \roman*]
  \item\label{cg_type:direct} $\mathcal{L}_m = \mathcal{K}_m$ and $\mathcal{L}_m = A\mathcal{K}_m$;
  \item\label{cg_type:transpose}$\mathcal{L}_m = \mathcal{K}_m(A^T,\mathbf{r}_0)$.
\end{enumerate}
Note that \cref{cg_type:direct} correspond to the residual and error projection methods. The former results in Arnoldi's method, as well as variants thereof like Full Orthogonalization Method (FOM), Incomplete Orthogonalization Method (IOM) and Direct Incomplete Orthogonalization Method (DIOM). The latter on the other hand results in the Generalized Minimum Residual Method (GMRES).

\subsection{Krylov subspaces}
\begin{definition}
  The grade of a vector $v$ with respect to a matrix $A$ is the lowest degree of the polynomial $q$ such that $q(A)v = 0$.
  \label{def:cg_grade}
\end{definition}
Consequently,
\begin{theorem}
  The Krylov subspace $\mathcal{K}_m$ is of dimension $m$ if and only if the grade $\mu$ of $v$ with respect to $A$ is not less than $m$ \cite[proposition 6.2]{iter_method_saad},
  \begin{equation*}
    \dim(\mathcal{K}_m) = m \iff \mu \geq m,
  \end{equation*}
  such that
  \begin{equation}
    \dim(\mathcal{K}_m) = \min \{m, \textrm{grade}(v)\}.
    \label{eq:cg_krylov_dimension}
  \end{equation}
  \label{th:cg_krylov_dimension}
\end{theorem}

\subsection{CG algorithm}
We can write the conjugate gradient method as \cref{alg:cg}.
\begin{algorithm}[H]
  \caption{Conjugate Gradient Method \cite[Algorithm 6.18]{iter_method_saad}}
  \begin{algorithmic}
    \State $\mathbf{r}_0 = b - A\mathbf{u}_0$, $p_0 = \mathbf{r}_0$, $\beta_0 = 0$
    \For{$j = 0, 1, 2, \dots, m$}
    \State $\alpha_j = (\mathbf{r}_j, \mathbf{r}_j) / (A p_j, p_j)$
    \State $\mathbf{u}_{j+1} = \mathbf{u}_j + \alpha_j p_j$
    \State $\mathbf{r}_{j+1} = \mathbf{r}_j - \alpha_j A p_j$
    \State $\beta_j = (\mathbf{r}_{j+1}, \mathbf{r}_{j+1}) / (\mathbf{r}_j, \mathbf{r}_j)$
    \State $p_{j+1} = \mathbf{r}_{j+1} + \beta_j p_j$
    \EndFor
  \end{algorithmic}
  \label{alg:cg}
\end{algorithm}
The Lanczos vectors are related through the Lanczos recurrence relation
\begin{equation}
  \label{eq:lanczos_recurrence}
  \eta_{j+1}(A)\mathbf{v}_{j+1} = A \mathbf{v}_j - \delta_j \mathbf{v}_j - \eta_j \mathbf{v}_{j-1},
\end{equation}
such that
\[
  T_m = \mathbf{v}_m^T A \mathbf{v}_m,
\]
where $T_m$ is the tridiagonal Hessenberg matrix given by
\begin{equation}
  T_m =
  \begin{pmatrix}
    \delta_1 & \eta_2   & 0        & \dots  & 0        \\
    \eta_2   & \delta_3 & \eta_3   & \dots  & 0        \\
    0        & \eta_3   & \delta_4 & \dots  & 0        \\
    \vdots   & \vdots   & \vdots   & \ddots & \eta_m   \\
    0        & 0        & 0        & \eta_m & \delta_m
  \end{pmatrix}.
  \label{eq:lanczos_tridiagonal}
\end{equation}
The following relations exist between the entries of $T_m$ and the CG coefficients $\alpha_j, \beta_j$
\begin{equation}
  \delta_{j+1} =
  \begin{cases}
    \frac{1}{\alpha_j} + \frac{\beta_{j-1}}{\alpha_{j-1}} & j > 0, \\
    \frac{1}{\alpha_0}                                    & j = 0,
  \end{cases}
  \label{eq:cg_hessenberg_delta}
\end{equation}
and
\begin{equation}
  \eta_{j+1} = \frac{\sqrt{\beta_{j-1}}}{\alpha_{j-1}}.
  \label{eq:cg_hessenberg_eta}
\end{equation}
Here we have used the definition of $T_m$ and the fact that the residuals are multiples of the Lanczos vectors $\mathbf{r}_j = \text{scalar} \times \mathbf{v}_j$ \cite[Equation 6.103]{iter_method_saad}.

\subsection{CG convergence rate}\label{sec:cg_convergence_rate}
It can be shown \cite[lemma 6.28 and theorem 6.29]{iter_method_saad} that the error of the $m^{\text{th}}$ iterate of the CG algorithm $\epsilon_m = x^* - \mathbf{u}_m$ minimizes the $A$-norm of the error in the affine Krylov subspace $\mathcal{K}_m(A, \mathbf{r}_0)$, that is
\begin{equation}
  ||(I - Aq_m(A))\epsilon_0||_A = \min_{q \in \mathcal{P}_{m-1}} ||(I - Aq(A))\epsilon_0||_A = \min_{r \in \mathcal{P}_{m-1}, r(0) = 1} ||r(A)\epsilon_0||_A,
  \label{eq:cg_convergence_rate}
\end{equation}
where the equality follows, since there exists an isomorphic mapping between the affine Krylov subspace and the polynomial space $\mathcal{P}_{m-1}$ of degree $m-1$ and the polynomial $tq(t)$ equals $0$ at $t=0$. The right-hand side can be further bounded by letting $\lambda_i, \xi_i$ be the eigenvalues of $A$ and the components of $\epsilon_0$ in the eigenvector basis of $A$, respectively. Then
\[
  ||r(A)\epsilon_0||_A = \sqrt{\sum_{i=1}^n |r(\lambda_i)|^2 |\xi_i|^2} \leq \max_{\lambda \in \sigma(A)} |r(\lambda)| ||\epsilon_0||_A,
\]
where $\sigma(A)$ is the spectrum of $A$. This gives
\begin{align*}
  ||e_m||_A                                                                                                                          &
  \leq \min_{r \in \mathcal{P}_{m-1}, r(0) = 1} \max_{\lambda \in \sigma(A)} |r(\lambda)| ||\epsilon_0||_A                                                                                                            \\
  \text{Chebyshev polynomial } C_m \text{, } \eta=\frac{\lambda_{\text{min}}}{\lambda_{\text{max}}-\lambda_{\text{min}}} \rightarrow & \frac{||\epsilon_0||_A}{C_m(1+2\eta)}                                          \\
                                                                                                                                     & \leq \frac{2||\epsilon_0||_A}{\left(1 + 2\eta + 2\sqrt{\eta(\eta+1)}\right)^m} \\
                                                                                                                                     & = \frac{2||\epsilon_0||_A}{\left(\sqrt{\eta} + \sqrt{\eta + 1}\right)^{2m}}    \\
                                                                                                                                     & = 2 \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa} + 1}\right)^m ||\epsilon_0||_A,
\end{align*}
where $\sigma(A) = [\lambda_{\text{min}}, \lambda_{\text{max}}]$ and $\kappa = \lambda_{\text{max}}/\lambda_{\text{min}}$ is the condition number of (the symmetric matrix) $A$. To sum up
\begin{theorem}
  The error of the $m^{\text{th}}$ iterate of the CG algorithm is bounded by
  \begin{equation}
    ||e_m|| \leq 2 \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa} + 1}\right)^m ||\epsilon_0||_A,
    \label{eq:cg_convergence_rate_bound}
  \end{equation}
  where $\kappa = \lambda_{\text{max}}/\lambda_{\text{min}}$ is the condition number of (symmetric matrix) $A$.
  \label{th:cg_convergence_rate_bound}
\end{theorem}
During the derivation of \cref{th:cg_convergence_rate}, we obtain the general expression for the error of the $m^{\text{th}}$ iterate of the CG algorithm
\[
  || e_m|| \leq \min_{r \in \mathcal{P}_{m}, r(0) = 1} \max_{\lambda \in \sigma(A)} |r(\lambda)| ||\epsilon_0||_A \\
\]
Now define,
\[
  \mathbf{r}_{\textrm{test}}(t) = \prod_{i=1}^m \frac{\lambda_i - t}{\lambda_i}.
\]
Note that $\mathbf{r}_{\textrm{test}}\in\mathcal{P}_m$, since it has degree $m$. Also, $\mathbf{r}_{\textrm{test}}(0) = 1$ and $\mathbf{r}_{\textrm{test}}(\lambda_i) = 0$ for $i = 1, 2, \dots, m$. Hence, $\mathbf{r}_{\textrm{test}}$ is a polynomial that satisfies the constraints of the minimization problem. We obtain for $m = N$ that
\[
  ||e_N||_A = ||\epsilon_0||_A \max_{\lambda \in \sigma(A)} |\mathbf{r}_{\textrm{test}}(\lambda)| = 0,
\]
which implies that CG converges in $N$ iterations in exact arithmetic. Furthermore, if there are only $k$ distinct eigenvalues, then the CG iteration terminates in at most $k$ iterations.

\subsection{Influence of eigenvalue distribution on CG convergence}\label{sec:cg_eigenvalue_distribution}
In the derivation of the convergence rate of the CG algorithm in \ref{th:cg_convergence_rate}, we used the Chebyshev polynomial to bound the error. However, we can find an expression of the error provided the eigendecomposition of $A$ is available. Suppose $A = VDV^T$, then $r(A) = I - Aq(A) = V(I - Dq(D))V^T = Vr(D)V^T$. Also note that $e_0 = x^* - \mathbf{u}_0 = A^{-1}b - \mathbf{u}_0 = A^{-1}\mathbf{r}_0$. As seen in \cref{eq:cg_convergence_rate}, the error of the $m^{\text{th}}$ iterate of the CG algorithm is given by
\begin{equation*}
  ||e_m||_A^2 = ||r_m(A)\epsilon_0||_A^2,
\end{equation*}
and
\begin{align*}
  ||r_m(A)\epsilon_0||_A^2 & = \epsilon_0^T r_m(A)^T A r_m(A) \epsilon_0                 \\
                           & = \epsilon_0^T V r_m(D) V^T V D V^T V r_m(D) V^T \epsilon_0 \\
                           & = (V^T\epsilon_0)^T r_m(D) D r_m(D) V^T \epsilon_0.
\end{align*}
We also have
\begin{align*}
  V^T\epsilon_0 & = V^T A^{-1} \mathbf{r}_0       \\
                & = V^T V D^{-1} V^T \mathbf{r}_0 \\
                & = D^{-1} \rho_0,
\end{align*}
where $\rho_0 = V^T \mathbf{r}_0$ is the initial residual in the eigenvector basis of $A$. Therefore,
\begin{align*}
  ||r_m(A)\epsilon_0||_A^2 & = \rho_0^T D^{-1} r_m(D) D r_m(D) D^{-1} \rho_0                 \\
                           & = \rho_0^T r_m(D) D^{-1} r_m(D)  \rho_0                         \\
                           & = \sum_{i=1}^n \frac{r_m(\lambda_i)^2}{\lambda_i} \rho_{0,i}^2,
\end{align*}
which gives
\begin{equation}
  ||e_m||_A^2 = \sum_{i=1}^n \frac{r_m(\lambda_i)^2}{\lambda_i} \rho_{0,i}^2.
  \label{eq:cg_error_eigenvalue}
\end{equation}

To obtain the residual polynomial $r_m$, we can use the recurrence relation between the Lanczos vectors and expressions for the Hessenberg matrix coefficients in \cref{eq:cg_hessenberg_delta,eq:cg_hessenberg_eta}. In particular,
\begin{align*}
  \frac{1}{\eta_{j+1}} \mathbf{v}_{j+1} & = A \mathbf{v}_j - \delta_j \mathbf{v}_j - \eta_j \mathbf{v}_{j-1} \\
                                        & = p_{j+1}(A) \mathbf{v}_1,
\end{align*}
where we define $p_{-1}(A) = 0, p_0(A) = I$. This gives
\begin{align*}
  \eta_{j+1}p_{j+1}(A)\mathbf{v}_1 & = A \mathbf{v}_j - \delta_j \mathbf{v}_j - \eta_j \mathbf{v}_{j-1},          \\
                                   & = \left( A p_j(A) - \delta_j p_j(A) - \eta_j p_{j-1}(A) \right)\mathbf{v}_1, \\
\end{align*}
and therefore
\begin{equation}
  p_{j+1}(A) = \frac{1}{\eta_{j+1}}\left( (A - \delta_j )p_j(A) - \eta_j p_{j}(A) \right).
  \label{eq:cg_lanczos_polynomial}
\end{equation}
Furthermore, we have the following relation between the residual polynomial and the Lanczos polynomial \cite[Section 3.2]{Meurant_Strakoš_2006}
\begin{equation}
  r_{j}(A) = (I-Aq_{j-1}(A))\mathbf{r}_0 = \frac{p_{j}(A)}{p_{j}(0)}\mathbf{r}_0.
  \label{eq:cg_residual_polynomial}
\end{equation}
This gives a way of calculating the residual polynomial $r_m$ and thereby the error of the $m^{\text{th}}$ iterate of the CG algorithm.

Additionally, the coefficients $c_i$ of the solution polynomial $q_m$ in \cref{eq:cg_approximate_solution} can be calculated. First we introduce a function that extracts the coefficients of a polynomial $p$
\begin{definition}
  Let $p(t) = \sum_{i=0}^n c_i t^i$ be a polynomial of degree $n$. Then, the function $\text{coeff}(p;i)$ extracts the $i^{\text{th}}$ coefficient of $p$ such that $\text{coeff}(p;i) = c_i$.
\end{definition}
Now using \cref{eq:cg_residual_polynomial}, we can write the solution polynomial as
\begin{align*}
                              & Aq_{m-1}(A) = I - r_m(A)                                                  \\
  r_m(\mathbf{0}) = I\implies & A\sum_{i=1}^{m-1} c_{i-1} A^i = -\sum_{i=1}^{m} \text{coeff}(r_m; i) A^i,
\end{align*}
which implies
\begin{equation}
  c_i = -\text{coeff}(r_m; i+1), \quad i = 0, 1, \dots, m-1.
  \label{eq:cg_solution_coefficients}
\end{equation}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{cg_convergence_behaviour.pdf}
  \caption{Residual polynomials resulting from successive CG iterations}
  \label{fig:cg_convergence_behaviour}
\end{figure}

The behavior of the residual polynomials is crucial for understanding the convergence properties of the CG method. In particular, the distribution of the eigenvalues of $A$ significantly affects the convergence rate, as illustrated in \cref{fig:cg_effect_of_eigenvalue_distribution}. For all plots the lowest and highest eigenvalue in \cref{fig:cg_effect_of_eigenvalue_distribution} are $\lambda_{\text{min}} = 0.1$, $\lambda_{\text{max}} = 0.9$ such that $f = \frac{\sqrt{\frac{\lambda_{\text{min}}}{\lambda_{\text{max}}}} - 1}{\sqrt{\frac{\lambda_{\text{min}}}{\lambda_{\text{max}}}} + 1}$ and the ratio $\frac{\|\mathbf{e}_m\|_A}{\|\mathbf{e}_0\|_A}$ is set to $\frac{10^{-6}}{\|\mathbf{u}_{\text{test}} - \mathbf{u}_0\|}$. The system size $N=360$ is kept small and the system matrix $A$ is diagonal so that it is numerically trivial to determine the exact solution $\mathbf{u}_{\text{test}}$. This results in an overall iteration bound 
\[
    m_{\text{classical}} = \left\lceil\log_f\left(\frac{10^{-6}}{2\|\mathbf{u}_{\text{test}} - \mathbf{u}_0\|}\right)\right\rceil = 26
\]
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{effect_of_eigenvalue_distribution.pdf}
  \caption{Plots of the last three CG residual polynomials for different eigenvalue distributions. $n_c$ indicates the number of clusters and $\sigma$ is the width of the cluster. The size of the system $N$ and the condition number $\kappa(A)$ are kept constant. $m$ indicates the number of iterations required for convergence.}
  \label{fig:cg_effect_of_eigenvalue_distribution}
\end{figure}
Hence, the number of iterations required for convergence depends on the specific clustering of the eigenvalues, as pointed out for example in \citeauthor[Section 2.3]{nonlinear_cg_Kelley_1995}.

From the behavior exhibited in figure \cref{fig:cg_effect_of_eigenvalue_distribution} as well as from \cref{th:cg_krylov_dimension} we can reason what the best and worst possible spectra for CG convergence are. That is, the best possible spectrum is one where eigenvalues are tightly clustered around distinct values, while the worst possible spectrum is one where the eigenvalues are evenly distributed across the whole range of the spectrum. This is illustrated in \cref{fig:cg_best_worst_spectra}.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{cg_convergence_extreme_spectra.pdf}
  \caption{Best and worst possible spectra for CG convergence}
  \label{fig:cg_best_worst_spectra}
\end{figure}

\subsection{Preconditioned CG} \label{sec:cg_preconditioning}
Suppose $M$ is some SPD preconditioner, then variants of CG can be derived by applying $M$ to the system of equations. The three main approaches are
\begin{enumerate}[label=\roman*,ref=preconditioner-type \roman*]
  \item\label{pcg_type:left} left
  \begin{align*}
    M^{-1}Ax & = M^{-1}b
  \end{align*}
  \item\label{pcg_type:right} right
  \begin{align*}
    AM^{-1}u & = M^{-1}b  \\
    x        & = M^{-1}u;
  \end{align*}
  \item\label{pcg_type:symmetric} symmetric or split
  \begin{align*}
    M              & = LL^T     \\
    x              & = L^{-T}u  \\
    L^{-1}AL^{-T}u & = L^{-1}b.
  \end{align*}
\end{enumerate}
Furthermore, all these variants are mathematically equivalent in some sense. Indeed, for the cases \cref{pcg_type:left} and \cref{pcg_type:right}, we can rewrite the CG algorithm using the $M-$ or $M^{-1}-$inner products, respectively. In either case the iterates are the same. For instance for the left preconditioned CG, we define $z_j = M^{-1}\mathbf{r}_j$. Note that $M^{-1}A$ is self-adjoint with respect to the $M-$inner product, that is
\[
  (M^{-1}Ax, y)_M = (Ax, y) = (x, Ay) = (x, M^{-1}Ay)_M.
\]
We use this to get a new expression for $\alpha_j$. To that end, we write
\begin{align*}
  0 & = (\mathbf{r}_{j+1}, \mathbf{r}_j)_M                                        \\
    & = (z_{j+1}, \mathbf{r}_j)                                                   \\
    & = (z_j - \alpha_j M^{-1}Ap_j, M^{-1}\mathbf{r}_j)_M                         \\
    & = (z_j, M^{-1}\mathbf{r}_j)_M - \alpha_j (M^{-1}Ap_j, M^{-1}\mathbf{r}_j)_M \\
    & = (z_j, z_j)_M - \alpha_j (M^{-1}Ap_j, z_j)_M                               \\
\end{align*}
and therefore
\[
  \alpha_j = \frac{(z_j, z_j)_M}{(M^{-1}Ap_j, z_j)_M}.
\]
Using $p_{j+1} = z_{j+1} + \beta_j p_j$ and A-orthogonality of the search directions with respect to $M-$norm $(Ap_j, p_k)_M = 0$ (), we can write
\[
  \alpha_j = \frac{(z_j, z_j)_M}{(M^{-1}Ap_j, p_j)_M}.
\]
Similarly, we can derive the equivalent expression of \cref{eq:cg_beta} as
\[
  \beta_j = \frac{(z_{j+1}, z_{j+1})_M}{(z_j, z_j)_M}.
\]
This gives the left preconditioned CG algorithm in \cref{alg:pcg_left}.
\begin{algorithm}[H]
  \caption{Left preconditioned CG \cite[Algorithm 9.1]{iter_method_saad}}
  \label{alg:pcg_left}
  \begin{algorithmic}
    \State $\mathbf{r}_0 = b - A\mathbf{u}_0$, $z_0 = M^{-1}\mathbf{r}_0$, $p_0 = z_0$, $\beta_0 = 0$
    \For{$j = 0, 1, 2, \dots, m$}
    \State $\alpha_j = (z_j, z_j)_M / (M^{-1}Ap_j, p_j)_M = (\mathbf{r}_j, z_j) / (Ap_j, p_j)$
    \State $\mathbf{u}_{j+1} = \mathbf{u}_j + \alpha_j p_j$
    \State $\mathbf{r}_{j+1} = \mathbf{r}_j - \alpha_j A p_j$
    \State $z_{j+1} = M^{-1}\mathbf{r}_{j+1}$
    \State $\beta_j = (z_{j+1}, z_{j+1})_M / (z_j, z_j)_M = (\mathbf{r}_{j+1}, z_{j+1}) / (\mathbf{r}_j, z_j)$
    \State $p_{j+1} = z_{j+1} + \beta_j p_j$
    \EndFor
  \end{algorithmic}
\end{algorithm}
Furthermore it can be shown that the iterates of CG applied to the system with \cref{pcg_type:symmetric} results in identical iterates \cite[Algorithm 9.2]{iter_method_saad}.

\section{Schwarz Methods}\label{sec:schwarz_methods}
The content of this section is largely based on chapters 1,2, 4 and 5 of \citeauthor{schwarz_methods_Dolean_2015} about Schwarz methods.

The original Schwarz method was a way of proving that a Poisson problem on some complex domain $\Omega$ still has a solution.
\begin{equation}
    \begin{array}{c}
        -\Delta u = f \text{ in } \Omega, \\
        u = 0 \text{ on } \partial \Omega.
    \end{array}
    \label{eq:poisson_problem}
\end{equation}
Existence is proved by splitting up the original complex domain in two (or more) simpler, possibly overlapping domains and solving the Poisson problem on each of these domains. The solution on the original domain is then the sum of the solutions on the subdomains. The method is named after Hermann Schwarz, who first introduced the method in 1869. The method has since been extended to more general problems and is now a popular method for solving partial differential equations.

\begin{definition}
    The Schwarz algorithm is an iterative method based on solving subproblems alternatively in domains $\Omega_1$ and $\Omega_2$. It updates $\left(u_1^n, u_2^n\right) \rightarrow\left(u_1^{n+1}, u_2^{n+1}\right)$ by
    \[
        \begin{array}{cc}
            \begin{aligned}
                -\Delta\left(u_1^{n+1}\right) & =f     &  & \text { in } \Omega_1,                                                 \\
                u_1^{n+1}                     & =0     &  & \text { on } \partial \Omega_1 \cap \partial \Omega, \quad \text{then} \\
                u_1^{n+1}                     & =u_2^n &  & \text { on } \partial \Omega_1 \cap \overline{\Omega_2} ;
            \end{aligned} &
            \begin{aligned}
                -\Delta\left(u_2^{n+1}\right) & =f         &  & \text { in } \Omega_2,                                   \\
                u_2^{n+1}                     & =0         &  & \text { on } \partial \Omega_2 \cap \partial \Omega,     \\
                u_2^{n+1}                     & =u_1^{n+1} &  & \text { on } \partial \Omega_2 \cap \overline{\Omega_1}.
            \end{aligned}
        \end{array}
    \]
    \label{def:schwarz_algorithm}
\end{definition}

The original Schwarz algorithm is sequential and, thereby, does not allow for parallelization. However, the algorithm can be parallelized. The Jacobi Schwarz method is a generalization of the original Schwarz method, where the subproblems are solved simultaneously and subsequently combined into a global solution. In order to combine local solutions into one global solution, an extension operator $E_i$, $i=1,2$ is used. It is defined as
\[
    E_i(v)=v \text { in } \Omega_i, \quad E_i(v)=0 \text { in } \Omega \backslash \Omega_i.
\]
Instead of solving for local solutions directly, one can also solve for local corrections stemming from a global residual. This is the additive Schwarz method (ASM). It is defined in algorithm \ref{alg:additive_schwarz}.
\begin{algorithm}[H]
    \caption{Additive Schwarz method \cite[Algorithm 1.2]{schwarz_methods_Dolean_2015}}
    \label{alg:additive_schwarz}
    \begin{algorithmic}
        \State Compute residual $r^n=f-\Delta u^n$.
        \State For $i=1,2$ solve for a local correction $v_i^n$:
        \[
            -\Delta v_i^n=r^n \text{ in } \Omega_i, \quad v_i^n=0 \text{ on } \partial \Omega_i
        \]
        \State Update the solution: $u^{n+1}=u^n+\sum_{i=1}^{2}E_i(v_i)^n$.
    \end{algorithmic}
\end{algorithm}

The restrictive additive Schwarz method (RAS) is similar to ASM, but differs in the way local corrections are combined to form a global one. In the overlapping region of the domains it employs a weighted average of the local corrections. In particular, a partition of unity $\chi_i$ is used. It is defined as
\[
    \chi_i(x)=
    \begin{cases}
        1,      & x \in \Omega_i \setminus \Omega_{3-i},                 \\
        0,      & x \in \delta \Omega_i \setminus \delta \Omega          \\
        \alpha, & 0 \leq \alpha \leq 1, x \in \Omega_i \cap \Omega_{3-i}
    \end{cases}
\]
such that for any function $w: \Omega \rightarrow \mathbb{R}$, it holds that
\[
    w = \sum_{i=1}^{2}E_i(\chi_i w_{\Omega_i}).
\]
The RAS algorithm is defined in algorithm \ref{alg:restrictive_additive_schwarz}.
\begin{algorithm}[H]
    \caption{Restrictive additive Schwarz method \cite[Algorithm 1.1]{schwarz_methods_Dolean_2015}}
    \label{alg:restrictive_additive_schwarz}
    \begin{algorithmic}
        \State Compute residual $r^n=f-\Delta u^n$.
        \State For $i=1,2$ solve for a local correction $v_i^n$:
        \[
            -\Delta v_i^n=r^n \text{ in } \Omega_i, \quad v_i^n=0 \text{ on } \partial \Omega_i
        \]
        \State Update the solution: $u^{n+1}=u^n + \sum_{i=1}^{2}E_i(\chi_i v_i^n)$.
    \end{algorithmic}
\end{algorithm}


\subsection{Schwarz methods as preconditioners}
Let $\mathcal{N}$ be set containing all indices of degrees of freedom in the domain $\Omega$ and $N_{\text{sub}}$ be the number of subdomains such that
\[
    \mathcal{N}=\sum_{i=1}^{N_{\text{sub}}} \mathcal{N}_i,
\]
$\mathcal{N}_i$ is the set of indices of degrees of freedom in the subdomain $\Omega_i$.

Furthermore, let $R_i\in\mathcal{R}^{\#\mathcal{N}_i \times \#\mathcal{N}}$, $R_i^T$ and $D_i$ be the discrete versions of the restriction, extension and partition of unity operators such that
\[
    \mathcal{R}^{\#\mathcal{N}}\ni U = \sum_{i=1}^{\mathcal{N}_{\text{sub}}} R_i^T D_i R_i U.
\]
Note that $D_i$ is a diagonal matrix where the entries are the values of the partition of unity function $\chi_i$ evaluated for each degree of freedom. Consider for instance, a multidimensional FEM problem, in which $\mathcal{T}$ is the triangulation of the domain $\Omega$ and $\mathcal{T}_i$ is the triangulation of the subdomain $\Omega_i$ such that \cite[Equation 1.27]{schwarz_methods_Dolean_2015}
\[
    \Omega_i = \cup_{\tau \in \mathcal{T}_i} \tau.
\]
In this case \cite[Equation 1.28]{schwarz_methods_Dolean_2015}
\[
    \mathcal{N}_i = \{k\in\mathcal{N}| \text{meas}(\text{supp}(\phi_k)\cap\Omega_i)>0\},
\]
and we can define
\[
    \mu_k = \#\{j| 1\leq j \leq N_{\text{sub}} \text{ and } k\in\mathcal{N}_j\}.
\]
Finally, this leads to
\begin{equation}
    (D_i)_{kk} = \frac{1}{\mu_k}, \ k \in \mathcal{N}_i.
    \label{eq:schwarz_partition_of_unity_FEM}
\end{equation}

Although the original Schwarz method is not a preconditioner, the ASM and RAS methods can be used as such. Originally the Schwarz method is a fixed point one \cite[Definitions 1.12 and 1.13]{schwarz_methods_Dolean_2015}
\[
    u^{n+1} = u^n + M^{-1}r^n, \ r^n = f - A u^n,
\]
where $M$ equals, but is not limited to, one of the following matrices;
\begin{subequations}
    \begin{align}
        M_{\text{ASM}} & = \sum_{i=1}^{N_{\text{sub}}} R_i^T (R_i A R_i^T)^{-1} R_i, \label{eq:ASM_preconditioner}     \\
        M_{\text{RAS}} & = \sum_{i=1}^{N_{\text{sub}}} R_i^T D_i (R_i A R_i^T)^{-1} R_i \label{eq:RAS_preconditioner}.
    \end{align}
\end{subequations}
Both $M_{\text{ASM}}$ and $M_{\text{RAS}}$ are symmetric and positive definite and can be used as preconditioners.

Optimized Schwarz methods and corresponding preconditioners can also be constructed by including more interface conditions (Robin or Neumann) in the subproblems. One such example is the Optimized Restrictive Additive Schwarz method (ORAS) discussed in \cite[Chapter 2]{schwarz_methods_Dolean_2015}.

\subsection{Convergence original Schwarz method} \label{sec:schwarz_convergence}
In this section the Schwarz problem stated in definition \ref{def:schwarz_algorithm} is solved in the one- and two-dimensional case. The convergence of the original Schwarz method is then discussed.

\subsubsection{1D case}
Let $L>0$ and the domain $\Omega = (0,L)$. The domain is split into two subdomains $\Omega_1 = (0,L_1)$ and $\Omega_2 = (l_2,L)$ such that $l_2\leq L_1$. Instead of solving for $u_{1,2}$ directly, we solve for the error $e^n_{1,2} = u^{n}_{1,2} - u_{|\Omega_i}$, which by linearity of the Poisson problem as well as the original Schwarz algorithm satisfies
\[
    \begin{array}{cc}
        \begin{aligned}
            -\frac{e_1^{n+1}}{d x^2} & = f           \text { in } (0,L_1), &                   \\
            e_1^{n+1}(0)             & = 0,                                & \quad \text{then} \\
            e_1^{n+1}(L_1)           & = e_2^n(L_1);                       &
        \end{aligned} &
        \begin{aligned}
            -\frac{e_2^{n+1}}{d x^2} & = f                \text { in } (l_2, L), & \\
            e_2^{n+1}(l_2)           & = e_1^{n+1}(l_2),                         & \\
            e_2^{n+1}(L)             & = 0.                                      &
        \end{aligned}
    \end{array}
\]
The solution to the error problem is
\[
    e_1^{n+1}(x) = \frac{x}{L_1}e_2^n(L_1), \quad e_2^{n+1}(x) = \frac{L-x}{L - l_2}e_1^{n+1}(l_2).
\]
Thes functions increase linearly from the boundary of the domain to the boundary of the overlapping region. The error at for instance $x = L_1$ is updated as
\[
    e_2^{n+1}(L_1) = \frac{1 - \delta/(L-l_2)}{1 + \delta/l_2} e_2^n(L_1),
\]
where $\delta = L_1 - l_2 > 0 $ is the overlap. The error is reduced by a factor of
\begin{equation}
    \rho_{\text{1D}} = \frac{1 - \delta/(L-l_2)}{1 + \delta/l_2},
    \label{eq:1D_Schwarz_convergence}
\end{equation}
which indicates the convergence becomes quicker as the overlap increases \cite[Section 1.5.1]{schwarz_methods_Dolean_2015}.

\subsubsection{2D case}
In the 2D case two half planes are considered $\Omega_1 = (-\infty, \delta)\times \mathbb{R}$ and $\Omega_2 = (\delta, \infty)\times \mathbb{R}$. Following the example of \citeauthor{schwarz_methods_Dolean_2015} the problem is
\begin{align*}
    -(\eta - \Delta) u & = f \text{ in } \mathbb{R}^2, \\
    u                  & \text{ bounded at infinity}.
\end{align*}
Proceeding in similar fashion as the one-dimensional case, the error $e^{n+1}_{1,2}$ can be solved for in the two subdomains. This is done via a partial Fourier transform of the problem in the y-direction yielding an ODE for the transformed error $\hat{e}^{n+1}_{1,2}$, which can be solved explicitly with the ansatz
\[
    \hat{e}^{n+1}_{1,2}(x, k) = \gamma_1(k) e^{\lambda_{+}(k) x} + \gamma_2(k) e^{\lambda_{-}(k) x},
\]
where $\lambda_{\pm}(k) = \pm \sqrt{k^2 + \eta}$. By using the interface conditions we find
\[
    \gamma_{i}^{n+1}(k) = \rho(k;\eta,\delta)^2 \gamma_{i}^{n-1}(k),
\]
such that the convergence factor is \cite[Equation 1.36]{schwarz_methods_Dolean_2015}
\begin{equation}
    \rho_{\text{2D}}(k;\eta,\delta) = e^{-\delta\sqrt{\eta + k^2}}
    \label{eq:2D_Schwarz_convergence}
\end{equation}
which indicates that the convergence is quicker as the overlap increases as before. Next to this, it also shows that the convergence is quicker for higher frequencies $k$.

\subsection{Need for a coarse space}
Following upon the results in the previous \cref{sec:schwarz_convergence} it is clear that the convergence of the Schwarz method not only depends on the extent of the overlap between various subdomains, but on the frequency components of the solution as well. In a general sense this means that low frequency modes need for instance at least $N_{\text{sub}}$ steps to travel from one end of a square domain to the other. This in turns causes plateaus in the convergence of the Schwarz method. To overcome this, we can perform a Galerkin projection of the error onto a coarse space. That is we solve
\[
    \min_{\beta} ||A(x + R_0^T\beta) - f||^2,
\]
where $Z$ is a matrix representing the coarse space. The solution to this problem is
\[
    \beta = (R_0 A R_0^T)^{-1} R_0 r,
\]
where $r = f - A x$ is the residual.

The coarse space $R_0$ can be constructed in various ways. The classical way is called the Nicolaides space \cite[Section 4.2]{schwarz_methods_Dolean_2015}, which uses the discrete partition of unity operators $D_i$ as examplified in \cref{eq:schwarz_partition_of_unity_FEM} to get
\begin{equation}
    R_0 = \sum_{i=1}^{N_{\text{sub}}} R_i^T D_i R_i.
    \label{eq:schwarz_nicolaides_coarse_space}
\end{equation}
Note that the course space has a block-diagional form.

Finally the coarse space correction term can be added to the Schwarz preconditioners \cref{eq:ASM_preconditioner,eq:RAS_preconditioner} to get the following preconditioners
\begin{subequations}
    \begin{align}
        M_{\text{ASM,2}} & = R_0^T (R_0 A R_0^T)^{-1} R_0 + \sum_{i=1}^{N_{\text{sub}}} R_i^T (R_i A R_i^T)^{-1} R_i , \label{eq:ASM_preconditioner_coarse}    \\
        M_{\text{RAS,2}} & = R_0^T (R_0 A R_0^T)^{-1} R_0 + \sum_{i=1}^{N_{\text{sub}}} R_i^T D_i (R_i A R_i^T)^{-1} R_i \label{eq:RAS_preconditioner_coarse}.
    \end{align}
\end{subequations}

\subsection{Two-level additive Schwarz method}
In this section the we will construct a coarse space for a Poisson problem with a constant scalar coefficient on arbitrary domain like in problem \ref{eq:poisson_problem}. However, the method is applicable to more general (highly) heterogeneous scalar problems, like the Darcy problem (see \cref{sec:schwarz_robust_coarse_spaces}). The coarse space is constructed using the eigenfunctions corresponding to the smallest $m_j$ eigenvalues resulting from a local eigenproblem in each subdomain $\Omega_j$. The coarse space is then constructed by taking the union of the $m_j$ eigenvectors corresponding to the smallest eigenvalues in each subdomain glued together by the partition of unity functions $\chi_j$. All of this can be found in \cite[Sections 5.1-5.5]{schwarz_methods_Dolean_2015}.

This coarse space is subsequently used to construct the two level additive Schwarz preconditioner, and bounds for its condition number are provided as well.

\subsubsection{Slowly convergent modes of the Dirichlet-to-Neumann map}
As seen in \cref{sec:schwarz_convergence} the local error in any subdomain in the Schwarz method satisfies the original problem without forcing, i.e. right hand side $f = 0$. At the interface the local error has a Dirichlet boundary condition that equals the error of the neighbouring subdomain. Additionally, the convergence factor, e.g. $\rho_{\text{2D}}$, depends on the frequency of the modes present in the local error. In particular, small frequencies appear to have slow convergence. The question thus becomes how to get rid of these small frequency modes in the local errors of all subdomains.

One possible answer is the so-called Dirichlet-to-Neumann map \cite[Definition 5.1]{schwarz_methods_Dolean_2015}
\begin{definition}
    (Dirichlet-to-Neumann map for a Poisson problem) For any function defined on the interface $u_{\Gamma_j}: \Gamma_j \mapsto \mathbb{R}$, we consider the Dirichlet-to-Neumann map
    \[
        \operatorname{DtN}_{\Omega_j}\left(u_{\Gamma_j}\right)=\left.\frac{\partial v}{\partial \mathbf{n}_j}\right|_{\Gamma_j},
    \]
    where $\Gamma_j:=\partial \Omega_j \backslash \partial \Omega$ and $v$ satisfies
    \begin{equation}
        \begin{array}{ccc}
            -\Delta v & =0            & \text { in } \Omega_j,                                \\
            v         & =u_{\Gamma_j} & \text { on } \Gamma_j,                                \\
            v         & =0            & \text { on } \partial \Omega_j \cap \partial \Omega .
        \end{array}
        \label{eq:dirichlet_to_neumann_map_subproblem}
    \end{equation}
    \label{def:dirichlet_to_neumann_map}
\end{definition}

The Dirichlet-to-Neumann map essentially solves for an error-like variable $v$ that satisfies the Dirichlet local interface (or global boundary) conditions. $\operatorname{DtN}$ then maps the interface condition to the normal derivative of $v$ on the interface, i.e. the Neumann condition. Now, as stated above and illustrated in \cite[Figure 5.2]{schwarz_methods_Dolean_2015} the low frequency modes of the error correspond to those modes that are nearly constant accross an interface, for which the Neumann condition is close to zero. So the problem of slowly convergent modes in the error of the Schwarz method is equivalent to a problem of finding eigenpairs of the $\operatorname{DtN}$ operator.

Hence we aim to solve the eigenvalue problem
\[
    \operatorname{DtN}_{\Omega_j}\left(v\right) = \lambda v,
\]
which can be reformulated in the variational form. To that end let $w$ be a test function that is zero on $\delta \Omega$. Multiply both sides of \cref{eq:dirichlet_to_neumann_map_subproblem} by $w$, integrate over $\Omega_j$ and apply Green's theorem to get
\[
    \int_{\Omega_j} \nabla v \cdot \nabla w - \lambda \int_{\Omega_j} \frac{\partial v}{\partial \mathbf{n}_j}w, \quad \forall w.
\]
Then, use the eigen property of $v$ and fact that $w$ is zero on $\delta \Omega$ to get the eigen problem in the variational form
\begin{equation}
    \text{Find } (v, \lambda) \text{ s.t. } \int_{\Omega_j} \nabla v \cdot \nabla w - \lambda \int_{\Gamma_j} vw = 0, \quad \forall w.
    \label{eq:dirichlet_to_neumann_map_eigenproblem}
\end{equation}

\subsubsection{FEM discretization}
The discretisation is done in the context of the finite element method. To that end we consider a triangulation $\mathcal{T}$ of the domain $\Omega$ and a partition of unity $\chi_j$. Denote by $V_{h,0} = V_h \cap H^1_0(\Omega)$ the space of piecewise continuous functions $v_h\in H^1_0(\Omega)$ with respect to $\mathcal{T}$. Let the basis of $V_{h,0}$ be given by $\{\phi_k\}_{\in\mathcal{N}}$. The FE formulation of the Poisson problem \cref{eq:poisson_problem} follows from the variational form
\[
    a(u_h, v_h) = (f, v_h), \quad \forall u_h, v_h \in V_{h,0},
\]
and is given by
\begin{equation}
    A\mathbf{u} = b, \quad A_{ij} = a(\phi_j, \phi_i), \ b_i = (f, \phi_i) \quad \forall i,j\in\mathcal{N}.
\end{equation}
Next to this we need a way of interpolating functions in $C(\Omega)$ to $V_{h}$. This is done by the interpolation operator $I_h: C(\Omega) \mapsto V_{h,0}$, which is defined by
\[
    \mathcal{I}_h v = \sum_{i\in\mathcal{N}} v(x_i) \phi_i,
\]
where $x_i$ are the nodes of the triangulation $\mathcal{T}$. $\mathcal{I}_h$ is stable with respect to the $a$-norm, that is
\[
    \| I_h(v) \|_a \leq C_{I_h} \| v \|_{a}.
\]

As before we partition $\Omega$ into $N_{\text{sub}}$ subdomains $\Omega_j$, which overlap each other by one or several layers of elements in the triangulation $\mathcal{T}$. We make the the following observations
\begin{enumerate}[label=\Roman*, ref=ASM observation \Roman*]
    \item\label{ASM_observation:basis_inclusion} For every degree of freedom $k\in\mathcal{N}$, there is a subdomain $\Omega_j$ such that $\phi_k$ has support in $\Omega_j$ \cite[Lemma 5.3]{schwarz_methods_Dolean_2015}.
    \item\label{ASM_observation:multiplicity_of_intersections} The maximum number of subdomains a mesh element can belong to is given by
    \[
        k_0 = \max_{\tau\in\mathcal{T}} \left (\#\{j|1\leq j\leq N_{\text{sub}} \text{ and } \tau \subset \Omega_j\} \right).
    \]
    \item\label{ASM_observation:number_of_colors} The minimum number of colors needed to color all subdomains so that no two adjacent subdomains have the same color is given by
    \[
        N_c \geq k_0
    \]
    \item\label{ASM_observation:overlapping_parameter} The minimum overlap for any subdomain $\Omega_j$ with any of its neihbouring subdomains is given by
    \[
        \delta_j = \inf_{x\in\Omega_j\setminus\cup_{i\neq j} \bar{\Omega}_i} \text{dist}(x, \partial \Omega_j\setminus\partial \Omega).
    \]
    \item\label{ASM_observation:partition_of_unity} The partition of unity functions $\{\chi_j\}_{j=1}^{N_{\text{sub}}}\subset V_h$ are such that
    \begin{enumerate}[label*=.\alph*]
        \item $\chi_j(x) \in [0,1], \quad \forall x\in\bar{\Omega}, j=1,\ldots,N_{\text{sub}}$,
        \item $\text{supp}(\chi_j) \subset \bar{\Omega}_j$,
        \item $\sum_{j=1}^{N_{\text{sub}}} \chi_j(x) = 1, \quad \forall x\in\bar{\Omega}$,
        \item $\|\nabla\chi_j(x)\| \leq \frac{C_{\chi}}{\delta_j}$,
    \end{enumerate}
    and are given by
    \[
        \chi_j(x) = I_h\left(\frac{d_j(x)}{\sum_{j=1}^{N_{\text{sub}}} d_j(x)}\right),
    \]
    where
    \[ 
        d_j(x) = 
            \begin{cases}
                \text{dist}(x, \partial \Omega_j), & x\in\Omega_j, \\
                0, & x\in\Omega\setminus\Omega_j.
            \end{cases}
    \]
    \item\label{ASM_observation:overlap_region} The overlap region for any subdomain is given by 
    \[
        \Omega_j^{\delta} = \{x\in\Omega_j| \chi_j < 1\}.
    \]
\end{enumerate}

The extension operator $E_j: V_{h,0}(\Omega_j) \rightarrow V_h$ is defined by 
\[
    V_h = \sum_{j=1}^{N_{\text{sub}}} E_j V_{h,0}(\Omega_j),
\]
which is gauranteed by \cref{ASM_observation:basis_inclusion}. 

Note that using the extension operator we can show that all the local bilinear forms are positive definite as
\[
    a_{\Omega_j}(v,w) = a(E_j v, E_j w) \geq \alpha \| E_j v \|_a^2, \quad \forall v,w\in V_{h,0}(\Omega_j),
\]
and $a$ is positive definite. 

Finally, we define the $a$-symmetric projection operators $\tilde{\mathcal{P}}_j: V_{h,0} \rightarrow V_h$ and $\mathcal{P}_j:V_h \rightarrow V_h$ defined by 
\begin{align*}
    a_{\Omega_j}(\tilde{\mathcal{P}}_j u, v_j) &= a(u, E_j v_j) \quad \forall v_j \in V_{h,0},\\
    \mathcal{P} &= E_j \tilde{\mathcal{P}}_j.
\end{align*}
then their matrix counterparts are given by 
\begin{align*}
    \tilde{P}_j &=  A_j^{-1} R_j^T A,\\
    P_j &= R_j^T A_j^{-1} R_j^T A,
\end{align*}
where $A_j = R_j A R_j^T$. From this we can construct the two-level additive Schwarz method as
\begin{equation}
    M_{\text{ASM,2}}^{-1} A = \sum_{j=1}^{N_{\text{sub}}} P_j.
    \label{eq:two_level_ASM_projections}
\end{equation}

\subsection{Convergence of two-level additive Schwarz}\label{sec:two_level_ASM_convergence}
In the following we denote
\[
    \mathcal{P}_{\text{ad}} = \sum_{j=1}^{N_{\text{sub}}} \mathcal{P}_j,
\]
and correspondingly,
\[
    P_{\text{ad}} = \sum_{j=1}^{N_{\text{sub}}} P_j.
\]

In the context of this thesis the two-level additive Schwarz method is used in combination with a Krylov subspace method (\cref{sec:cg}), in which case convergence rate depends on the entire spectrum of eigenvalues (\cref{sec:cg_eigenvalue_distribution}). However, an upperbound for the convergence rate (\cref{sec:cg_convergence_rate}) can be derived from the condition number of $P_{\text{ad}}$ via \cref{eq:cg_convergence_rate}. 

Using the fact that $P_{\text{ad}}$ is symmetric with respect to the $a$-norm, we can write
\[
    \kappa(P_{\text{ad}}) = \frac{\lambda_{\text{max}}}{\lambda_{\text{min}}},
\]
where
\[
    \lambda_{\text{max}} = \sup_{v\in V_h} \frac{a(\mathcal{P}_{\text{ad}})}{a(v,v)}, \quad \lambda_{\text{min}} = \inf_{v\in V_h} \frac{a(\mathcal{P}_{\text{ad}})}{a(v,v)}.
\]

Additionally, we can employ the $a$-orthogonality of the projection operators to get
\[
   \frac{a(\mathcal{P}_j u, u)}{\|u\|_a^2} = \frac{a(\mathcal{P}_j u, \mathcal{P}_j u)}{\|u\|_a^2} \leq 1.
\]
Going further, we can pose that the projection operators defined by the sum of projection operators $\mathcal{P}_j$ of like-colored subdomains are $a$-orthogonal to each other. This is due to the fact that the partition of unity functions $\chi_j$ are such that they are zero on the interface of like-colored subdomains (see \cref{ASM_observation:number_of_colors}). To that end, define
\[
    \mathcal{P}_{\text{\Theta}_i} = \sum_{j\in\Theta_i} \mathcal{P}_j,
\]
where $\Theta_i$ is the set of indices of subdomains with color $i$ and $i = 1, \dots, N_c$. Then, we can write \cite[Lemma 5.9]{schwarz_methods_Dolean_2015}
\begin{align*}
    \lambda_{\text{max}}(\mathcal{P}_{\text{ad}}) & = \sup_{v\in V_h} \sum_{i=1}^{N_c} \frac{a(\mathcal{P}_{\text{\Theta}_i} v, v)}{a(v,v)} \\
    & \leq \sum_{i=1}^{N_c} \sup_{v\in V_h} \frac{a(\mathcal{P}_{\text{\Theta}_i} v, v)}{a(v,v)} \\
    & \leq N_c + 1,
\end{align*}
where the extra one comes from the coarse projection operator $\mathcal{P}_0$. Note that this bound can be made sharper by using \cref{ASM_observation:multiplicity_of_intersections} to get $\lambda_{\text{max}}(\mathcal{P}_{\text{\Theta}_i}) \leq k_0 + 1$.

On the other hand, it can be shown that the minimum eigenvalue satisfies provided that $v\in V_h$ admits a $C_0$-stable decomposition \cite[Theorem 5.11]{schwarz_methods_Dolean_2015}
\[
    \lambda_{\text{min}}(\mathcal{P}_{\text{ad}}) \geq C_0^{-2}.
\] 

Finally, we can write the condition number of the two-level additive Schwarz preconditioner as
\begin{equation}
    \kappa(P_{\text{ad}}) \leq \left( N_c + 1 \right) C_0^2.
    \label{eq:two_level_ASM_condition_number}
\end{equation}

The value of $C_0$ depends on the projection operator $\Pi_j$ to the chosen coarse space $V_0$ for each subdomain.
\begin{enumerate}[label=\Roman*., ref=ASM coarse space \Roman*]
    \item\label{ASM_coarse_space:nicolaides} \textbf{Nicolaides coarse space} The projection operator is defined as
    \begin{equation}
        \Pi_j^{\text{Nico}}u = \begin{cases}
            (\frac{1}{|\Omega_j|}\int_{\Omega_j} u)\mathbf{1}_{\Omega_j}, & \delta\Omega_j \cap \delta \Omega = \emptyset, \\
            0, & \text{otherwise},
        \end{cases}
        \label{eq:nicolaides_coarse_space_projection}
    \end{equation}
    which gives rise to the following basis functions in $V_{h,0}$
    \[
        \Phi^{\text{Nico}}_j = I_h(\chi_j \mathbf{1}_{\Omega_j}).
    \]
    Then,
    \[
        V_0 = \text{span}\{\Phi^{\text{Nico}}_j\}_{j=1}^{N_{\text{sub}}},
    \]
    and
    \[
        \dim V_0 = \text{the number of floating subdomains},
    \]
    that is the number of subdomains that are not connected to the boundary of the domain $\Omega$. In this case \cite[Theorem 5.16]{schwarz_methods_Dolean_2015}
    \begin{equation}
        C_0^{2} = \left(8 + 8 C_{\chi}^2 \max_{j=1}^{N_{\text{sub}}}\left[C_P^2 + C^{-1}_{\text{tr}}\frac{H_j}{\delta_j}\right]k_0 C_{I_h}(k_0 + 1) + 1\right),
        \label{eq:c0_nicolaides}
    \end{equation}
    where $H_j$ is the diameter of the subdomain $\Omega_j$, $C_p$ the Poincaré constant following from \cite[Lemma 5.18]{schwarz_methods_Dolean_2015} and $C_{\text{tr}}$ is the trace constant.
    \item\label{ASM_coarse_space:local_eigenfunctions} \textbf{Local eigenfunctions coarse space} The projection operator is defined as
    \[
        \Pi_j^{\text{spec}}u = \sum_{k=1}^{m_j} a_{\Omega_j}(u, v^{(j)}_k) v^{(j)}_k,
    \]
    where $v^{(j)}_k$ is the $k^{\text{th}}$ eigenfunction resulting from the eigenproblem in \cref{eq:dirichlet_to_neumann_map_eigenproblem}. The basis functions in $V_{h,0}$ are then given by
    \[
        \Phi^{\text{spec}}_{j,k} = I_h(\chi_j v^{(j)}_k),
    \]
    resulting in the coarse space
    \[
        V_0 = \text{span}\{\Phi^{\text{spec}}_{j,k}\}_{j=1,k=1}^{N_{\text{sub}},m_j},
    \]
    with dimension
    \[
        \dim V_0 = \sum_{j=1}^{N_{\text{sub}}} m_j.
    \]
    In this case \cite[Theorem 5.17]{schwarz_methods_Dolean_2015}
    \begin{equation}
        C_0^{2} = \left(8 + 8 C_{\chi}^2 \max_{j=1}^{N_{\text{sub}}}\left[C_P^2 + C^{-1}_{\text{tr}}\frac{1}{\delta_j\lambda_{m_j +1}}\right]k_0 C_{I_h}(k_0 + 1) + 1\right).
      \label{eq:c0_local_eigenfunctions}
    \end{equation}
\end{enumerate}