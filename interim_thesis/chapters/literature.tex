\chapter{Related Work}\label{ch:literature}\newpage
\section{The spectral gap arising in Darcy problems}\label{sec:spectral_gap_darcy}
In a Darcy problem, high-contrast $ \mathcal{C}(x) $ (e.g., $ 10^6 $ in conductive channels vs.\ $ 10^{-6} $ in barriers) means flow concentrates in high-permeability regions, while low-permeability zones resist flow. This heterogeneity introduces modes (eigenvectors) that are nearly constant or slowly varying over low-$ \mathcal{C}(x) $ regions, contributing small eigenvalues to $ A $. These modes represent ``trapped'' or ``isolated'' behaviors disconnected by the contrast.

Small eigenvalues arise when the bilinear form $ a(u, v) = \int_{\Omega} \mathcal{C}(x) \nabla u \cdot \nabla v \, dx $ yields low energy for certain test functions $ v $. In high-contrast cases, if $ \mathcal{C}(x) $ is tiny in a subdomain, $ \nabla u $ must be large there to balance the equation, but FEM basis functions often cannot resolve this without fine meshes. Instead, coarse bases produce modes where energy is minimized, leading to eigenvalues close to zero. This is exacerbated as contrast grows, adding more such modes.

High-contrast $ \mathcal{C}(x) $ can split the spectrum into clusters: large eigenvalues tied to high-$ \mathcal{C}(x) $ regions (where gradients dominate) and small eigenvalues tied to low-$ \mathcal{C}(x) $ regions (where flow stagnates). In \cite{msfem_for_darcy_Efendiev2011} note that standard FEM misses these small eigenvalues unless enriched bases capture fine-scale features.

\section{Tailored coarse spaces for high-contrast problems}\label{sec:tailored_coarse_spaces}
Various methods for constructing a coarse space that are both scalable and robust to high contrast in a problem coefficient.

\subsection{MsFEM} 
The Multiscale Finite Element Method (MsFEM), as presented in \cite{msfem_coarse_space_Graham_2007}, constructs a coarse space based on five key assumptions (C1-C5). These assumptions ensure stability and accuracy by defining how the coarse space interacts with the fine-scale problem. Local coarse grid basis functions are obtained by solving the homogeneous version of the system equation, meaning they do not include external forcing terms. The construction of these basis functions requires specific boundary conditions, categorized as M1-M4, which control their behavior at interfaces. The method distinguishes between linear and oscillatory boundary conditions for local problems, affecting the resulting coarse space. Coarse grid basis functions are computed as harmonic extensions of basis functions restricted to edges or faces, ensuring continuity across subdomains. The restriction operator $R_0$ is then derived from these basis functions, as given in Equation 2.12 of \cite{msfem_coarse_space_Graham_2007}. Additionally, the method introduces robustness indicators, $\pi(\alpha)$ and $\gamma(\alpha)$, which quantify the stability of the coarse space and its effectiveness in capturing fine-scale features.
 
\subsection{ACMS}
The Approximate Component Mode Synthesis (ACMS) method, detailed in \cite{acms_coarse_space_Heinlein2018}, introduces a separation of scales with fine and coarse triangulations, denoted as $\mathcal{T}_h$ and $\mathcal{T}_H$. The coarse problem is decomposed into two components: $u_c = u_I + u_{\Gamma}$, where $u_I$ represents the inner part and $u_{\Gamma}$ the interface contribution. This extends MsFEM by incorporating vertex-specific, edge-specific, and fixed-interface basis functions, where MsFEM corresponds solely to the vertex-specific functions. The vertex-specific basis functions are defined as harmonic extensions of trace values on the interface set $\Gamma$. Edge-specific basis functions, on the other hand, arise from an eigenvalue problem defined on an edge $e$, while fixed-interface basis functions correspond to eigenmodes of an eigenvalue problem within a coarse element $T$.
    
ACMS supports two types of coarse spaces, depending on whether Dirichlet (DBC) or Neumann (NBC) boundary conditions are applied. Under DBCs, MsFEM basis functions are combined with edge-specific basis functions that match on a shared edge $e_{ij}$ between subdomains $\Omega_i$ and $\Omega_j$. These functions are constructed from the harmonic extension of eigenmodes defined on the edge $e_{ij}$, with a scaled bilinear form on the right-hand side. Only eigenmodes corresponding to eigenfrequencies below a set tolerance are retained. With NBCs, both MsFEM and edge-specific basis functions are modified. MsFEM functions remain defined on an edge $e_{ij}$ and satisfy a Kronecker-delta vertex condition but are now obtained via a generalized eigenvalue problem on a slab of width $kh$, denoted $\eta^{kh}_{ij}$. The edge-specific functions are similarly defined through a generalized eigenvalue problem on the slab but without enforcing DBCs. Solving these eigenvalue problems can be computationally efficient by employing mass matrix lumping techniques.
 
\subsection{(R)GDSW}
The Generalized Dryja-Smith-Widlund (GDSW) method, introduced by \cite{gdsw_coarse_space_Dohrmann2008}, partitions the computational domain into non-overlapping subdomains and further divides degrees of freedom (DOFs) into interior and interface nodes. The only required input for the method is a coarse space $G$, whose columns span the rigid body modes of the subdomains. The restriction operators $R_{\Gamma}$ and $R_I$ project onto interface and interior DOFs, respectively, with subdomain-specific versions such as $R_{\Gamma_j}$.

The coarse solution on the interface set is given by 
\[
    u_{0,\Gamma} = \sum_{j}^{N_{\text{sub}}} R^T_{\Gamma_j} G_{\Gamma_j} q_j = \Phi_{\Gamma} q,
\]
where $q$ represents the coarse space coefficients. The complete coarse solution is then given by 
\[
    u_0 = R^T_{\Gamma} u_{0,\Gamma} + R^T_{I} u_{0,I},
\]
where $R_I$ is derived from energy-minimizing extensions of $u_{0,\Gamma}$ into the subdomain interiors. Applying the energy minimization principle to the potential $u_0 A u_0$ leads to the definition of $\Phi_I$, which governs the interior contributions.
\subsection{AMS}
The Algebraic Multiscale (AMS) method, introduced in ... studied in \cite{ams_coarse_space_comp_study_Alves2024}, also relies on domain decomposition into non-overlapping subdomains, followed by a further subdivision of interface nodes into edge, vertex, and face nodes (in 3D). The method eliminates lower diagonal blocks in the system matrix to facilitate efficient computation. Like (R)GDSW, AMS employs the energy minimization principle to obtain $\Phi_I$, ensuring an optimal coarse space representation.

\section{CG convergence in case of non-uniform spectra}\label{sec:cg_nonuniform_spectra}
A lot of work has been done on the convergence rate of the CG method, especially in the context of non-uniform spectra. The following papers provide valuable insights into this topic.

First, in \cite{cg_sharpened_convrate_Axelsson1976} a clever use of Chebyshev polynomials is demonstrated to obtain a sharpened CG iteration bound for two kinds of eigenspectra. This technique is further developed in \cref{sec:cg_sharpened_convrate}, providing additional insights into the convergence behavior of CG.

Second, in \cite{cg_convrate_Strakos1991} the convergence rate of CG is investigated in the case of non-uniform spectra. The authors show that the convergence rate strongly depends on the eigenvalue distribution of the matrix \(A\). By examining a parametrized class of matrices, they study how small perturbations in eigenvalues impact CG performance. Their experiments reveal that there exists a critical value of a parameter at which the number of iterations required for convergence greatly exceeds the degrees of freedom \(N\), even when the condition number is small (\(K=100\)) and \(N\) is small (e.g., 12 or 24). Moreover, this critical value shifts with increasing precision, so that higher precision reduces the impact of rounding errors. The study further shows that the CG behavior is consistent across different algorithm variants (standard CG, SYMMQL, Jacobi acceleration, etc.). These results suggest that certain eigenvalue distributions make CG highly sensitive to numerical errors, and they underline the importance of preconditioners that can modify the eigenvalue distribution to improve CG robustness in practical applications.

Third, in \cite{cg_superlinear_Beckermann2001} the authors provide a proof of CG's superlinear convergence. They explain why the conjugate gradient method exhibits faster (superlinear) convergence than the classical bound suggests. In particular, they derive a new asymptotic error bound that is sharper than the standard estimate. This new bound is expressed via the integral \cite[Equation 1.8]{cg_superlinear_Beckermann2001}:
\begin{equation}
    \frac{1}{n} \log \left(\min_{r\in\mathcal(P)_m, r(0)=1}\max_{\lambda \sigma(A)}|r(\lambda)|\right) \lesssim-\frac{1}{t} \int_0^t g_{S(\tau)}(0) d \tau,
    \label{eq:cg_superlinear_Beckermann2001}
\end{equation}
where $t = \frac{m}{N}$, $S(\tau)$ is a family of sets determined by the eigenvalue distribution and $g_S$ is the Green function for the complement of $S$ with pole at $\infty$. As shown by \cref{eq:cg_superlinear_Beckermann2001} and theorem 2.1 in \cite{cg_sharp_bound_Beckermann2001}, the error is bounded by a term that decreases more quickly as the number of iterations increases. Under additional separation conditions among eigenvalues (see Theorem 2.2), the bound is proven to be asymptotically sharp. Moreover, for matrices with equidistant eigenvalues, an explicit formula \cite[Corollary 3.2 and Equation 3.11]{cg_superlinear_Beckermann2001} confirms the improved rate and aligns with observed CG error curves. These findings help to explain why, in practice, CG converges faster than predicted by traditional condition number bounds.

Fourth, in \cite{cg_sharp_bound_Beckermann2001} the authors present a proof of the sharpness of the CG iteration bound \cref{eq:cg_superlinear_Beckermann2001}. The paper shows that one cannot beat the asymptotic error estimate bound obtained earlier. It analyzes a strategy in which zeros of the polynomial are set at all eigenvalues outside a chosen set \(S\). The authors prove that any such polynomial cannot yield a better asymptotic error bound than the one given by the earlier formula. They use a constrained energy problem from logarithmic potential theory to define the optimal set \(S(t)\). Under conditions that are natural for problems arising from discretized PDEs on the eigenvalue distribution, the bound is demonstrated to be sharp, and the discussion includes cases where equality in the bound is reached.

Finally, in \cite{cg_superlinear_rhs_Beckermann2002} the authors extend the results of \cite{cg_superlinear_Beckermann2001} to cases where the eigenvalue distribution is asymptotically uniform. They show that even when the asymptotic distribution equals an equilibrium distribution, the CG method can exhibit superlinear convergence. In this work, the superlinearity stems from the particular choice of the right-hand side \(b\). The authors present a family of examples based on the finite difference discretization of the one-dimensional Poisson problem, where they observe superlinear convergence according to the chosen right-hand sides.
