\chapter{Methodology}\label{ch:methods}
The methods described in this chapter are adapted from the ideas discussed in \cite[Section 4]{cg_sharpened_convrate_Axelsson1976}. Therein \citeauthor{cg_sharpened_convrate_Axelsson1976} presents a sharpened CG iteration bound for two particular eigenspectra, which are described in \cref{sec:cg_sharpened_convrate}. The sharpened bound is then generalized to multiple clusters in \cref{sec:multiple_clusters}. The results are then compared with the classical CG iteration bound in \cref{sec:cg_sharpened_convrate_numerical_experiments}. Finally, the implications of the sharpened bound for the research questions are discussed in \cref{sec:cg_sharpened_convrate_implications}.

\section{Two cluster case}\label{sec:cg_sharpened_convrate}
On the eigenspectrum of $A$, consider two intervals $[a, b]$ and $[c, d]$ with $0 < a < b < c < d$ such that all eigenvalues of $A$ are contained in the union of these two intervals. Additionally, we have $\kappa(A) = \frac{d}{a}$. We treat the following two cases simultaneously
\begin{equation}
    \sigma_1(A) = [a,b] \bigcup [c,d]
    \label{eq:two_clusters}
\end{equation}

\begin{equation}
    \sigma_2(A) = [c,d] \bigcup_{\substack{i=1 \\ \lambda_i \in [a,b]}}^{N_{\text{tail}}} \lambda_i
    \label{eq:one_cluster_with_tail}
\end{equation}
where $N_{\text{tail}}$ is the number of eigenvalues in the tail. The first case is a two-cluster eigenspectrum, while the second case has one cluster and a tail of eigenvalues.
 
The CG error \cref{eq:cg_error_bound} suggests we look for a polynomial $r_{m_2}$ of degree $m_2$ that satisfies the constraints of the minimization problem in order to find a iteration bound $m < m_2$. In other words, we do not solve the minimization problem directly, but we make a clever selection of the polynomial $r_{m_2}$ that satisfies the constraints. As a consequence, the actual minimizing polynomial might require a lower degree $m$ to satisfy the same relative error tolerance $\epsilon$.

\citeauthor{cg_sharpened_convrate_Axelsson1976} suggests we use not one monolithic residual polynomial function, but a multiplication of two residual polynomial functions $\hat{r}^{(i)}_p(x)$ and $\hat{r}_{m_2-p}(x)$ for the two clusters. The superscript $^{(i)}$ corresponds to the two eigenspectra described above. The residual polynomial functions are defined using \cref{def:scaled_chebyshev_polynomial} and $\gamma = 0$ as follows
\begin{equation}
    \hat{r}^{(i)}_p (x)
    \begin{cases}
        \hat{C}_p&, \text{if } i = 1\\
        \overset{p}{\underset{i=1}{\prod}} (1 - x/\lambda_i)&, \text{if } i = 2, p = N_{\text{tail}}\\
    \end{cases}
    \label{eq:residual_polynomial_rm}
\end{equation}
and
\begin{equation}
    \hat{r}_{{m_2}-p} (x) = \frac{C_{m_2-p} \left(\frac{d + c - 2x}{d - c}\right)}{C_{m_2-p}\left(\frac{d + c}{d - c}\right)},
    \label{eq:residual_polynomial_rpm}
\end{equation}
Indeed, the product $r_{m_2} = \hat{r}_p \hat{r}_{m_2-p} \in \mathcal{P}_{m_2}$. Hence, we can use the residual polynomial functions to bound the error at the $m^{\text{th}}$ iterate. Now, we obtain the following intermediate bounds
\begin{subequations}
    \begin{align}
        \max_{\lambda \in [a,b]} |r_{m_2}(\lambda)| \leq \max_{\lambda \in [a,b]} |\hat{r}^{(i)}_p(\lambda)| \max_{\lambda \in [a,b]} |\hat{r}_{m_2-p}(\lambda)| &\leq \max_{\lambda \in [a,b]} |\hat{r}^{(i)}_p(\lambda)|, \ \text{and} \label{eq:residual_polynomial_bound_ab}\\
        \max_{\lambda \in [c,d]} |r_{m_2}(\lambda)| \leq \max_{\lambda \in [c,d]} |\hat{r}^{(i)}_p(\lambda)| \max_{\lambda \in [c,d]} |\hat{r}_{m_2-p}(\lambda)| &\leq \max_{\lambda \in [c,d]} |\hat{r}_{p}(\lambda)|/C_{m_2-p}\left(\frac{d+c}{d-c}\right) \label{eq:residual_polynomial_bound_cd}
    \end{align}
\end{subequations}
where the first result follows from the fact that $|\hat{r}_{m-p}(x)| < 1 \ \forall x \in [a,b]$ and the second result from 
\[
    \left|C_{m-p}\left(\frac{d+c -2x}{d-c}\right)\right| < 1 \ \forall x \in [c,d].
\]

Furthermore, using the equality \cref{eq:chebyshev_polynomial_approximation}, we have
\begin{equation}
    \frac{1}{C_{k}\left(\frac{z_1 + z_2}{z_1 - z_2}\right)} \leq 2 \left(\frac{\sqrt{z_2} - \sqrt{z_1}}{\sqrt{z_2} + \sqrt{z_1}}\right)^k, \text{ for } z_1 > z_2 > 0 \text{ and } k \in \mathbb{N}^+,
    \label{eq:chebyshev_polynomial_bound}
\end{equation}
and
\begin{equation}
    \max_{\lambda \in [a,b]} |\hat{r}^{(i)}_p(\lambda)| \leq
    \begin{cases}
        2\left(\frac{\sqrt{b}-\sqrt{a}}{\sqrt{b}+\sqrt{a}}\right)^p=\eta_1 &, \text{if } i = 1,\\
        \left(\frac{b}{a}-1\right)^p=\eta_2 &, \text{if } i = 2, p = N_{\text{tail}},
    \end{cases}
    \label{eq:residual_polynomial_bound_ab_i}
\end{equation}
Note that if $i=1$ we can determine $p$ by requiring that the maximum of the residual polynomial function $\hat{r}^{(i)}_p$ in $[a,b]$ is equal to $\epsilon$. This gives the following equation
\begin{equation}
    p \geq \left\lfloor\frac{1}{2}\sqrt{\frac{b}{a}}\ln{\frac{2}{\epsilon}} + 1\right\rfloor
    \label{eq:chebyshev_degree_p}
\end{equation}
Also note that for $i=2$ $\hat{r}^{(2)}_p(\lambda) = 0 < \epsilon$ for all eigenvalues $\lambda \in [a,b]$.

Next, $\hat{r}^{(i)}_p$ in $[c,d]$ is bounded by its maximum value within $[a,b]$ multiplied by the polynomial that is the fastest growing polynomial in $\mathcal{P}_{p}$ outside- and bounded below 1 within $[a,b]$. This polynomial is again the (transformed) Chebyshev polynomial $C_{p}\left(\frac{2x - b - a}{b - a}\right)$. Therefore,
\begin{equation*}
    \max_{\lambda \in [c,d]} |\hat{r}^{(i)}_p(\lambda)| \leq \eta_i C_{p}\left(\frac{2d - b - a}{b + a}\right),
\end{equation*}
with $\eta_i$ as defined in \cref{eq:residual_polynomial_bound_ab_i}.

At this point we have ensured that $\max_{\lambda \in [a,b]}|r_{m_2}|$ is bounded by $\epsilon$ using \cref{eq:residual_polynomial_bound_ab}. So it remains to bound $\max_{\lambda \in [c,d]}|r_{m_2}|$ in \cref{eq:residual_polynomial_bound_cd}. Using above results we can write 
\begin{equation*}
    \max_{\lambda \in [c,d]} |r_{m_2}(\lambda)| < \epsilon,
\end{equation*}
if we require that
\begin{equation}
    \eta_i C_{p}\left(\frac{2d - b - a}{b - a}\right) /C_{m_2-p}\left(\frac{d+c}{d-c}\right) < \epsilon.
    \label{eq:relative_error_bound_mp}
\end{equation}
Using that for $x_1, x_2, x_3 \in \mathbb{R}^+$ with $x_1 > x_3$ and $z = \frac{x_1 - x_2}{x_3}$
\begin{align*}
    C_p(z) & \leq \left(z + \sqrt{z^2 - 1}\right)^p \\
    & = \left( \frac{x_1 - x_2}{x_3} + \sqrt{ \left[\frac{x_1 - x_2}{x_3}\right]^2 -1}\right)^p \\
    & \leq \left( \frac{x_1}{x_3} + \sqrt{ \left[\frac{x_1}{x_3}\right]^2 - 1}\right)^p \\
    & \leq \left( \frac{2x_1}{x_3}\right)^p,
\end{align*}
and substituting $x_1 = 2d$, $x_2 = b + a$ and $x_3 = b - a$ we obtain the following inequality
\begin{equation}
    \eta_i \left(\frac{4d}{b-a} \right)^p /C_{m_2-p}\left(\frac{d+c}{d-c}\right) < \epsilon. 
    \label{eq:chebyshev_degree_p_bound}
\end{equation}
Moreover,
\begin{align*}
    \eta_i \left(\frac{4d}{b-a}\right)^p &= 
    \begin{cases}
        2\left(\frac{\sqrt{b} - \sqrt{a}}{\sqrt{b} + \sqrt{a}} \frac{4d}{b-a}\right)^p &, \text{if } i = 1\\
        \left(\frac{b - a}{a}\frac{4d}{b-a}\right)^p &, \text{if } i = 2,
    \end{cases}\\
    &=
    \begin{cases}
        2\left(\frac{4d}{b + 2\sqrt{ab} + a}\right)^p &, \text{if } i = 1\\
        \left(\frac{4d}{a}\right)^p &, \text{if } i = 2,
    \end{cases}\\
    &\leq 2
    \begin{cases}
        \left(\frac{4d}{b}\right)^p &, \text{if } i = 1\\
        \left(\frac{4d}{a}\right)^p &, \text{if } i = 2,
    \end{cases}
\end{align*}
We can therefore require that the bound in \cref{eq:chebyshev_degree_p_bound} is satisfied if we have
\[
    1/C_{m_2-p}\left(\frac{d+c}{d-c}\right) \leq \frac{\epsilon}{2\left( \frac{4d}{e_i}\right)^p},
\]
where 
\[
    e_i = \begin{cases}
        b &, \text{if } i = 1\\
        a &, \text{if } i = 2.\\
    \end{cases}
\]
Again using \cref{eq:chebyshev_polynomial_bound} and solving for the degree $m_2 - p$ we obtain
\[
    m_2 - p \geq \frac{1}{2}\sqrt{\frac{d}{c}}\left(\ln{\epsilon} + p \ln{\frac{4d}{e_i}}\right),
\]
which leads to the following bound for the number of iterations \cite[Equation 4.4]{cg_sharpened_convrate_Axelsson1976}
\begin{equation}
    m_2=\left\lfloor\frac{1}{2} \sqrt{\frac{d}{c}} \ln (2 / \epsilon)+\left(1+\frac{1}{2} \sqrt{\frac{d}{c}} \ln (4 d / e_i)\right) p\right\rfloor,
    \label{eq:cg_iteration_bound_2_clusters}
\end{equation}
where 
\[
    1 \leq p \leq \min\left(\left\lfloor\frac{1}{2}\sqrt{\frac{b}{a}}\ln{\frac{2}{\epsilon}} + 1 \right\rfloor, N_{\text{tail}}\right).
\]

\section{Performance criteria}\label{sec:cg_sharpened_convrate_th_performance}
In this section we compare the new bound in \cref{eq:cg_iteration_bound_2_clusters} (case $i=1$) to the (approximated) classical bound in \cref{eq:cg_convergence_rate_bound_iterations_approx}. We will see that the new bound is not absolutely sharper than the classical one. However, we will derive an approximate, though accurate, criterion (see \cref{eq:threshold_inequality_explicit_expansion}) that allows us to discern under what conditions the new bound \textit{is} sharper.

To that end, we introduce a measure of performance as the ratio of the number of iterations predicted by the classical bound to that predicted by the sharpened bound
\begin{equation}
    P = \frac{m}{m_2}.
    \label{eq:performance_ratio}
\end{equation}
Next to this, we introduce the left and right cluster condition numbers $\kappa_l = \frac{b}{a} > 1$ and $\kappa_r = \frac{d}{b} > 1$. Rewriting the bound in \cref{eq:cg_iteration_bound_2_clusters} then gives
\[
    m_2(\kappa, \kappa_l, \kappa_r)=\left\lfloor\frac{\sqrt{\kappa_r}}{2} \ln (2 / \epsilon)+\left(1+\frac{\sqrt{\kappa_r}}{2} \ln \left(\frac{4\kappa}{\kappa_l}\right)\right) p\right\rfloor.
\]
Then, substituting $p$ gives
\begin{equation}
    m_2(\kappa, \kappa_l, \kappa_r)=\left\lfloor
        1 
        + \frac{\sqrt{\kappa_r}}{2}\ln\left(\frac{4\kappa}{\kappa_l}\right)
        + \frac{1}{2}\ln\left(\frac{2}{\epsilon}\right)\left(
            \sqrt{\kappa_l}
            + \sqrt{\kappa_r}
            + \sqrt{\kappa_l\kappa_r}\ln\left(\frac{4\kappa}{\kappa_l}\right)
        \right)
    \right\rfloor.
\end{equation} 

\subsection{Uniform performance ratio}
In order to see that the new bound is not absolutely sharper than the classical one, we determine the minimum value of $P$. We know that the product of two lower order Chebyshev polynomials is not optimal for a uniform eigenspectrum, cf. the proof of Chebyshev optimality outlined in \cref{th:minmax_polynomial}. Therefore, we assume that the minimum value of $P$ is attained for the case of a uniform eigenspectrum, i.e. $a<b=c<d$, and we can set $\kappa=\kappa_l\kappa_r$, which yields
\begin{equation}
    m_2(\kappa=\kappa_l\kappa_r, \kappa_l, \kappa_r)=\left\lfloor
        1 
        + \frac{\sqrt{\kappa_r}}{2}\ln\left(4\kappa_r\right)
        + \underbrace{\frac{\sqrt{\kappa}}{2}\ln\left(\frac{2}{\epsilon}\right)}_{=m(\kappa)}\left(
            \underbrace{
                \frac{1}{\sqrt{\kappa_l}}
                + \frac{1}{\sqrt{\kappa_r}}
                + \ln\left(4\kappa_r\right)
            }_{:= q(\kappa_l, \kappa_r)}
        \right)
    \right\rfloor.
    \label{eq:cg_iteration_bound_2_clusters_uniform}
\end{equation}
We recognize the classical CG bound as the factor in front of the last term in \cref{eq:cg_iteration_bound_2_clusters_uniform}. Now, the performance ratio $P$ in \cref{eq:performance_ratio} satisfies
\begin{equation}
    P(\kappa=\kappa_l\kappa_r, \kappa_l, \kappa_r) = P_{\text{uniform}}(\kappa_l, \kappa_r) = \left(\frac{1 + \frac{1}{2}\sqrt{\kappa_r}\ln(4\kappa_r)}{m(\kappa_l\kappa_r)} + q(\kappa_l, \kappa_r)\right)^{-1},
    \label{eq:performance_ratio_uniform}
\end{equation}
which can be expanded as
\[
    P_{\text{uniform}}(\kappa_l, \kappa_r) = q(\kappa_l, \kappa_r)^{-1} - \frac{1 + \frac{1}{2}\sqrt{\kappa_r}\ln(4\kappa_r)}{m(\kappa)q(\kappa_l, \kappa_r)^2} + \mathcal{O}\left(\frac{1}{m(\kappa)^2q(\kappa_l, \kappa_r)^3}\right).
\]
Next, we can expand $q(\kappa_l, \kappa_r)^{-1}$ as follows
\[
    q(\kappa_l, \kappa_r)^{-1} = \frac{1}{\ln(4\kappa_r)} - \frac{1}{\ln(4\kappa_r)^2}\left(\frac{1}{\sqrt{\kappa_l}} + \frac{1}{\sqrt{\kappa_r}}\right) + \mathcal{O}\left(\frac{1}{(\ln(4\kappa_r))^3}\left(\frac{1}{\sqrt{\kappa_l}} + \frac{1}{\sqrt{\kappa_r}}\right)^2\right),
\]
which gives the performance for a uniform eigenspectrum as
\begin{align}
    P_{\text{uniform}}(\kappa_l, \kappa_r) &= \frac{1}{\ln(4\kappa_r)} \notag \\
    &\quad - \frac{1}{\ln(4\kappa_r)^2}\left(\frac{1}{\sqrt{\kappa_l}}\left[1 + \frac{1}{\ln\left(\frac{2}{\epsilon}\right) + 1}\right] + \frac{1}{\sqrt{\kappa_r}} + \frac{2}{\sqrt{\kappa_l\kappa_r}\ln\left(\frac{2}{\epsilon}\right) + 1}\right) \notag \\
    &\quad + \mathcal{O}\left(\frac{1}{\ln(4\kappa_r)^3}\right).
    \label{eq:performance_ratio_uniform_expansion}
\end{align}

Equation \ref{eq:performance_ratio_uniform_expansion} shows that the uniform (minimum) performance $P_{\text{uniform}}(\kappa_l, \kappa_r)$ depends in its leading order term entirely on the right clusters' condition number $\kappa_r$. That is
\[
    P_{\text{uniform}}(\kappa_l, \kappa_r) \lesssim \frac{1}{\ln(4\kappa_r)} = \tilde{P}_{\text{uniform}}(\kappa_r).
\]
Only for small $\kappa_l = \frac{\kappa}{\kappa_r}$ does the second order term become significant. Additionally, \cref{eq:performance_ratio_uniform_expansion} shows that for increasing $\kappa_r$ we expect a decreasing minimum performance. Finally, we indeed find that the new bound $m_2(\kappa,\kappa_l,\kappa_r)$ is not absolutely sharper than the classical bound $m(\kappa)$.

Moreover, we can say that
\begin{equation}
    P_{\text{uniform}}(\kappa_l, \kappa_r) \lesssim \tilde{P}_{\text{uniform}}(\kappa_r)  \leq P(\kappa, \kappa_l, \kappa_r) \lesssim 1,
    \label{eq:performance_ratio_no_improvement_bounds}
\end{equation}
as long as
\begin{equation}
    \kappa_l\kappa_r \leq \kappa \lesssim T_h(\kappa_l, \kappa_r),
    \label{eq:performance_ratio_uniform_bound}
\end{equation}
where $T_h(\kappa_l, \kappa_r)$ is derived in \cref{sec:performance_threshold} as \cref{eq:threshold_inequality_explicit}.

\subsection{Performance threshold}\label{sec:performance_threshold}
Now that we have established that the new bound is not absolutely sharper than the classical one, we can determine the conditions under which the new bound is sharper. We require $P\geq 1$ to derive such conditions. This gives the following inequality
\[
    \frac{m}{m_2} \geq 1 \Rightarrow m(\kappa) \geq m_2(\kappa, \kappa_l, \kappa_r),
\]
which can be rewritten as
\begin{equation}
    \sqrt{\frac{\kappa}{\kappa_l\kappa_r}} \geq \ln\left(4\frac{\kappa}{\kappa_l}\right) + \frac{1}{\sqrt{\kappa_r}} + \frac{1}{\sqrt{\kappa_l}}\left(\frac{1 + \ln\left(\frac{4\kappa}{\kappa_l}\right)}{\ln\left(\frac{2}{\epsilon}\right)}\right).
    \label{eq:threshold_inequality}
\end{equation}
At this point we assume that $\kappa_l \gg 1$. This allows us to neglect the last term in \cref{eq:threshold_inequality}. Next to simplifying the equation itself, this reduces the number of variables. Indeed, introducing a new parameter for the \textit{spectral width} $s = \frac{\kappa}{\kappa_l}$, dropping the last term in \cref{eq:threshold_inequality} and setting $c_r = \frac{1}{\sqrt{\kappa_r}}$ gives
\begin{equation}
    c_r\sqrt{s} \geq \ln\left(4s\right) + c_r.
    \label{eq:threshold_inequality_s}
\end{equation}
We use the Lambert $\mathrm{W}$ function to solve the equality in \cref{eq:threshold_inequality_s} for $s$. In order to do so, we transform the above equation into the form $x = y \mathrm{e}^y$ with $y = -\frac{2\sqrt{s}}{c_r}$ and $x = -\frac{4}{c_r\exp\left(\frac{c_r}{2}\right)}$. Then, we can write
\begin{align*}
    -\frac{c_r\sqrt{s}}{2} = y = W_{-1}(x) = W_{-1}\left(-\frac{c_r}{4\exp\left(\frac{c_r}{2}\right)}\right), \quad x \in \left(\frac{1}{\mathrm{e}}, 0 \right],
\end{align*}
where $W_{-1}$ is the first negative branch of the Lambert $\mathrm{W}$ function. The condition on $x$ ensures that $W_{-1}$ evaluates to a real number. Finally, after substituting $c_r = \frac{1}{\sqrt{\kappa_r}}$ we obtain an explicit expression for the spectral width $s$ in terms of the right cluster condition number $\kappa_r$
\[
    s(\kappa, \kappa_l) \geq 4\kappa_r W_{-1}\left(-\frac{1}{4\sqrt{\kappa_r}\exp\left(\frac{1}{2\sqrt{\kappa_r}}\right)}\right)^2,
\]
or, in terms of the original condition numbers
\begin{equation}
    \kappa \gtrsim 4\kappa_l\kappa_r W_{-1}\left(-\frac{1}{4\sqrt{\kappa_r}\exp\left(-\frac{1}{2\sqrt{\kappa_r}}\right)}\right)^2 = T_h(\kappa_l, \kappa_r).
    \label{eq:threshold_inequality_explicit}    
\end{equation}
The evaluation of the Lambert $\mathrm{W}$ function is not a trivial task and often requires numerical methods for accurate computation \cite{evaluation_of_the_lambert_w_function_Corless1996}. Luckily, there exists an expansion of $\mathrm{W}_{-1}(x)$ for $x\rightarrow0^-$ \cite[Equation 4.19]{evaluation_of_the_lambert_w_function_Corless1996}. Let $x$ be as above and set $l = \ln(-x)$ and $L = \ln(-L)$, then
\begin{equation}
    \kappa \gtrsim 4\kappa_l\kappa_r \left(L - l + \frac{l}{L}\right)^2 + \mathcal{O}\left(\frac{\kappa_l\kappa_rl^4}{L^4}\right).
    \label{eq:threshold_inequality_explicit_expansion}    
\end{equation}

\subsection{Performance plot}
Figure \ref{fig:two_cluster_bound_performance} visualizes all the findings regarding the performance of the two-cluster bound $m_2$. 
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{performance_vs_condition_number.pdf}
    \caption{Performance ratio of the classical bound $m$ from \cref{eq:cg_convergence_rate_bound_iterations_approx} and the new two-cluster bound $m_2$ from \cref{eq:cg_iteration_bound_2_clusters} as a function of the global condition number $\kappa$ for right cluster condition number $\kappa_r = 2$ (\textbf{left}) and $\kappa_r = 10^3$ (\textbf{right}). All plots contain graphs for spectra with $\kappa_l = 1, 10, 10^2, 10^3, 10^4$. The plots also contain a red, shaded no-improvement region for which $P < 1$, a diagonally hashed region resembling the performance bounds from \cref{eq:performance_ratio_no_improvement_bounds}, a red, dotted line resembling the second order expansion of the uniform performance ratio from \cref{eq:performance_ratio_uniform_expansion} and both the exact and approximate $\kappa$-threshold values for each $\kappa_l$ graph from \cref{eq:threshold_inequality_explicit,eq:threshold_inequality_explicit_expansion}, respectively.}
    \label{fig:two_cluster_bound_performance}
\end{figure}

\section{Generalization to multiple clusters}\label{sec:multiple_clusters}
At this point we assume that we are dealing with an eigenspectrum of the form $\sigma_1(A)$, i.e. we are only treating case 1. In \cref{sec:cg_sharpened_convrate_implications}, it is reasoned that this is indeed a very applicable case for \cref{prob:elliptic_problem_discretized}.

In this case, the technique outlined in \cref{sec:cg_sharpened_convrate} starts at the left most cluster $[a,b]$, finds the Chebyshev degree $p_1=p$ satisfying \cref{eq:chebyshev_degree_p}, moves to the neighboring cluster $[c,d]$ and finds the Chebyshev degree $p_2 = m_2 - p$ satisfying \cref{eq:relative_error_bound_mp}. Rewriting \cref{eq:relative_error_bound_mp} gives the following equation for $p_2$:
\begin{equation}
    \frac{1}{C_{p_2}\left(\frac{d+c}{d-c}\right)} \leq \frac{\epsilon}{{C}^{(1)}_{p_1}(d)} = \epsilon_2,
    \label{eq:chebyshev_degree_p_prime}
\end{equation}
where
\[
    C^{(1)}_{p_1}(x) = C_{p_1}\left(\frac{b + a - 2x}{b - a}\right) /C_{p_1}\left(\frac{b+a}{b-a}\right),
\]
is the Chebyshev polynomial corresponding to the first cluster.

Suppose there is a third cluster to the right of $[c,d]$, i.e. $[e,f]$. We can repeat the process and find the Chebyshev degree $p_3$ satisfying a similar as \cref{eq:chebyshev_degree_p_prime} for the third cluster. 
\[
    \frac{1}{C_{p_3}\left(\frac{f+e}{f-e}\right)} \leq \frac{\epsilon}{C^{(1)}_{p_1}(f)C^{(2)}_{p_2}(f)} = \epsilon_3,
\]
This leads to the general equation for the Chebyshev degree $p_i$ of the $i^{\text{th}}$ cluster $[a_i, b_i]$
\begin{equation}
    \frac{1}{C_{p_i}\left(\frac{b_i + a_i}{b_i - a_i}\right)} \leq \frac{\epsilon}{\prod_{j=1}^{i-1} C^{(j)}_{p_j}(b_i)} = \epsilon_i.
    \label{eq:chebyshev_degree_p_i}
\end{equation}

Due to the large range of the Chebyshev polynomials $\tilde{C}_p$ computer is likely to result in floating point number overflow during calculation of the denominator of \cref{eq:chebyshev_degree_p_i}. Instead, we first apply \cref{eq:chebyshev_polynomial_bound} and introduce the cluster condition numbers $\kappa_i = \frac{b_i}{a_i}$, where $i$ is the index of the cluster. We can then rewrite \cref{eq:chebyshev_degree_p_i} as follows
\begin{equation*}
    p_i  =  \left\lceil\ln{\frac{\epsilon_i}{2}} / \ln{\frac{\sqrt{\kappa_i} - 1}{\sqrt{\kappa_i} + 1}}\right\rceil, \\
\end{equation*}
and
\begin{align*}
    \ln{\frac{\epsilon_i}{2}} & = \ln{\frac{\epsilon}{2}} - \sum_{j=1}^{i-1} \ln{C^{(j)}_{p_j}(b_i)}. \\
\end{align*}
Let $z^{(i,j)}_1 = \frac{b_j + a_j - 2b_i}{b_j - a_j}$ and $z^{(j)}_2 = \frac{b_j + a_j}{b_j - a_j}$ then
\begin{equation*}
    \ln{C^{(j)}_{p_j}(b_i)} = \ln{C_{p_j}(z^{(i,j)}_1)} - \ln{C_{p_j}(z^{(j)}_2)}.
\end{equation*}
We have, using the definition of the Chebyshev polynomial
\begin{equation}
    \ln{C_{p_j}(z^{(i,j)}_1)} \lessapprox p_j \ln{\left[z^{(i,j)}_1 - \sqrt{\left(z^{(i,j)}_1\right)^2 - 1}\right]} - \ln{2},
    \label{eq:chebyshev_polynomial_bound_z1}
\end{equation}
and
\begin{equation}
    \ln{C_{p_j}(z^{(j)}_2)} \gtrapprox p_j \ln{\left[z^{(j)}_2 + \sqrt{\left(z^{(j)}_2\right)^2 - 1}\right]} - \ln{2},
    \label{eq:chebyshev_polynomial_bound_z2}
\end{equation}
both of which become more accurate approximations as $z,m\rightarrow\infty$. Introducing 
\begin{align*}
    \zeta^{(i,j)}_1 &= z^{(i,j)}_1 - \sqrt{\left(z^{(i,j)}_1\right)^2 - 1}, \\
    \zeta^{(j)}_2 &= z^{(j)}_2 + \sqrt{\left(z^{(j)}_2\right)^2 - 1}, \text{ and}\\
    f_i &= \frac{\sqrt{\kappa_i} - 1}{\sqrt{\kappa_i} + 1},
\end{align*}
with $\kappa_i$ the $i^{\text{th}}$ cluster condition number, and substituting the inequalities \ref{eq:chebyshev_polynomial_bound_z1} and \ref{eq:chebyshev_polynomial_bound_z2} back into the bound for $p_i$ gives
\begin{align*}
    p_i &\leq \left\lceil\frac{\ln{\frac{\epsilon}{2}} - \sum_{j=1}^{i-1} p_j\left(\ln{\zeta^{(i,j)}_1} - \ln{\zeta^{(j)}_2} \right)}{\ln{f_i}}\right\rceil \\
    &= \left\lceil\log_{f_i}{\frac{\epsilon}{2}} - \sum_{j=1}^{i-1} p_j\left(\log_{f_i}{\zeta^{(i,j)}_1} - \log_{f_i}{\zeta^{(j)}_2} \right)\right\rceil\\
    &= \left\lceil\log_{f_i}{\frac{\epsilon}{2}} - \sum_{j=1}^{i-1} p_j\log_{f_i}{\frac{\zeta^{(i,j)}_1}{\zeta^{(j)}_2}} \right\rceil
\end{align*}
Note that in general $\zeta^{(i,j)}_1 < \zeta^{(j)}_2$ and hence $\log_{f_i}{\left(\frac{\zeta^{(j)}_2}{\zeta^{(i,j)}_1}\right)} > 0$. This prompts us to write
\begin{equation}
    p_i \leq \left\lceil\log_{f_i}{\frac{\epsilon}{2}} + \sum_{j=1}^{i-1} p_j\log_{f_i}{\frac{\zeta^{(j)}_2}{\zeta^{(i,j)}_1}} \right\rceil
    \label{eq:chebyshev_degree_p_i_explicit}
\end{equation}
Evidently, adding more clusters to the left of the interval $[a_i,b_i]$ increases the degree $p_i$ of the Chebyshev polynomial. Next to this, \cref{eq:chebyshev_degree_p_i_explicit} reduces to the classical CG iteration bound \cref{eq:cg_convergence_rate_bound_iterations} for a single cluster when $i = N_{\text{clusters}} = 1$.

Equation \ref{eq:chebyshev_degree_p_i_explicit} gives us a way to calculate the Chebyshev degree $p_i$ of the $i^{\text{th}}$ cluster $[a_i,b_i]$ in terms of the Chebyshev degrees of the previous clusters. To obtain a bound on the number of iterations for the CG method we sum the Chebyshev degrees of all the clusters
\begin{equation}
    m_2 = \sum_{i=1}^{N_{\text{clusters}}} p_i
    \label{eq:cg_iteration_bound_multiple_clusters}
\end{equation}