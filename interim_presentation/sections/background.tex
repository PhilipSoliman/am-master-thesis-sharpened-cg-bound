\begin{frame}[label=background]{Structure Background}
    \frametitle{Structure}
    \begin{itemize}
        \item Research Questions
        \item {\color{tud grapefruit}Background \& Literature}
        \item Preliminary Results
        \item Outlook
    \end{itemize}
\end{frame}

\footerinfootnotestrue
\begin{frame}[label=background,fragile]{Background: CG method}
    \frametitle{Background \& Literature}
    \framesubtitle{Conjugate gradient method}
    \begin{algorithm}[H]
        \begin{algorithmic}
            \State $\mathbf{r}_0 = \mathbf{b} - A\mathbf{u}_0$, $\mathbf{p}_0 = \mathbf{r}_0$, $\beta_0 = 0$
            \For{$j = 0, 1, 2, \dots, m$}
            \State $\alpha_j = (\mathbf{r}_j, \mathbf{r}_j) / (A \mathbf{p}_j, \mathbf{p}_j)$
            \State $\mathbf{u}_{j+1} = \mathbf{u}_j + \alpha_j \mathbf{p}_j$
            \State $\mathbf{r}_{j+1} = \mathbf{r}_j - \alpha_j A \mathbf{p}_j$
            \State $\beta_j = (\mathbf{r}_{j+1}, \mathbf{r}_{j+1}) / (\mathbf{r}_j, \mathbf{r}_j)$
            \State $\mathbf{p}_{j+1} = \mathbf{r}_{j+1} + \beta_j \mathbf{p}_j$
            \EndFor
        \end{algorithmic}
        \caption{Conjugate Gradient Method \cite{iter_method_saad}}
    \end{algorithm}
\end{frame}

\footerinfootnotesfalse
\begin{frame}[label=background,fragile]{Background: CG explained}
    \frametitle{Background \& Literature}
    \framesubtitle{Conjugate gradient method: residual polynomial}
    \begin{columns}[T,onlytextwidth]
        \begin{column}{.5\textwidth}
            \begin{itemize}
                \item<+-> Iterative, projection method onto a Krylov subspace $\mathcal{K}_m(A_0, \mathbf{r}_0)$ given by
                \begin{equation*}
                     \text{span}\{\mathbf{r}_0, A\mathbf{r}_0, A^2\mathbf{r}_0, \dots, A^{m-1}\mathbf{r}_0\}
                \end{equation*}
                \item<+-> Approximate solution can be expressed as
                \begin{equation*}
                    \mathbf{u}_m = \mathbf{u}_0 + \sum_{i=0}^{m-1} c_i A^i \mathbf{r}_0 = \mathbf{u}_0 + \alert<3>{q_{m-1}}(A)\mathbf{r}_0
                \end{equation*}
                \item<4> Minimize residual polynomial on eigenvalues of $A$
            \end{itemize}
        \end{column}
        \begin{column}{.5\textwidth}
            \only<3-4>{%
                \begin{figure}
                    \centering
                    \includegraphics[width=0.8\textwidth]{two_cluster_respoly.png}
                    \caption{Residual polynomial $r_m(\lambda) = 1 - \lambda \alert<3>{q_{m-1}}(\lambda)$}
                \end{figure}
            }
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}[label=background,fragile]{CG convergence}
    \frametitle{Background \& Literature}
    \framesubtitle{Conjugate gradient method: convergence}
    \begin{itemize}
        \item<1-> Classical (condition number) convergence bound:
        \begin{theorem}
            The error of the $m^{\text{th}}$ iterate of the CG algorithm is bounded by
            \begin{equation*}
            ||\mathbf{e}_m|| \leq 2 \left(\frac{\sqrt{\kappa}-1}{\sqrt{\kappa} + 1}\right)^m ||\mathbf{e}_0||_A,
            \end{equation*}
            where $\kappa = \lambda_{\text{max}}/\lambda_{\text{min}}$ is the condition number of (symmetric matrix) $A$.
        \end{theorem}
        \item<2-> Only sharp for \alert<2>{uniform} eigenvalue distributions!
        \begin{equation*}
            ||\mathbf{e}_m||_A \leq \min_{r \in \mathcal{P}_{m-1}, r(0) = 1} \max_{\lambda \in [\lambda_{\text{min}}, \lambda_{\text{max}}]} |r(\lambda)| ||\mathbf{e}_0||_A \overset{\alert<2>{\text{uniform } \sigma(A)}}{=} \frac{\|\mathbf{e}_0\|}{C_m\left(\frac{\kappa + 1}{\kappa - 1}\right)}
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}[label=background,fragile]{CG spectra}
    \frametitle{Background \& Literature}
    \framesubtitle{Conjugate gradient method: spectral distribution}
    Setting $\lambda_{\text{min}} = 0.1$ and $\lambda_{\text{max}} = 0.9$ gives $m_{\text{classical}} = 26$. \only<1>{\textcolor{tud grapefruit}{Worst case}}\only<2>{\textcolor{tud green}{Best case}} distribution:
    \only<1>{%
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{cg_convergence_extreme_spectra_cluster1.pdf}
            \caption{CG convergence for uniform spectrum.}
        \end{figure}
    }
    \only<2>{%
        \begin{figure}
            \centering
            \includegraphics[width=0.7\textwidth]{cg_convergence_extreme_spectra_cluster0.pdf}
            \caption{CG convergence for spectrum with two distinct eigenvalues.}
        \end{figure}
    }
\end{frame}

\footerinfootnotestrue
\begin{frame}[label=background,fragile]{Literature: non-uniform spectra}
    \frametitle{Background \& Literature}
    \framesubtitle{Conjugate gradient method: non-uniform spectra}
    \begin{columns}[T,onlytextwidth]
        \begin{column}{.5\textwidth}
            \begin{itemize}
                \item<1-> Clusterpoint distribution ($0 < \rho \leq 1$)\cite{cg_convrate_Strakos1991}
                \begin{align*}
                    &\lambda_i = \lambda_1 + \frac{i-1}{N-1}(\lambda_N - \lambda_1)\rho_i^{N-i}, \\ 
                    &\text{For }i = 1, \dots, N
                \end{align*}
                \item<2-> $\rho = 1$ gives uniform distribution
                \item<3-> $\rho = 0$ gives two distinct eigenvalues
                \item<4-> $0<\rho<1$ gives a spectrum with a clusterpoint at $\lambda_1$.
            \end{itemize}
        \end{column}
        \begin{column}{.45\textwidth}
            \only<4>{%
                \begin{figure}
                    \centering
                    \includegraphics[width=\textwidth]{clusterpoint_distribution_Strakos.png}
                    \caption{CG iterations versus $\rho$}
                \end{figure}
            }
        \end{column}
    \end{columns}
\end{frame}

\footerinfootnotestrue
\begin{frame}[label=background,fragile]{Literature: CG Convergence for Non-uniform Spectra}
    \frametitle{Background \& Literature}
    \framesubtitle{Conjugate gradient method: non-uniform spectra}
    \begin{columns}[T,onlytextwidth]
        \begin{column}{.5\textwidth}
            \only<1-2>{%
                \begin{itemize}
                    \item<1-2> Let $0<a<b<c<d<1$ and consider\cite{cg_sharpened_convrate_Axelsson1976}
                    \begin{itemize}
                        \item<2> Two disjoint clusters
                        \begin{equation*}
                            \sigma_1(A) = [a,b] \bigcup [c,d]
                        \end{equation*}
                        \item<2> Cluster with tail
                        \begin{equation*}
                            \sigma_2(A) = [c,d] \bigcup_{\substack{i=1 \\ \lambda_i \in [a,b]}}^{N_{\text{tail}}} \lambda_i
                        \end{equation*}
                    \end{itemize}
                \end{itemize}
            }
            \only<3-8>{%
            \begin{itemize}
                \item<3-> $A$-norm optimality of CG
                \begin{equation*}
                    \|\mathbf{e}_m\|_A = \min_{r \in \mathcal{P}_{m}, r(0) = 1} \sum_{i=1}^{m}\frac{r(\lambda_i)^2}{\lambda_i}\rho_{0,i}^2
                \end{equation*}
                \item<4-> We look for $r_{\bar{m}} = \hat{r}_p \hat{r}_{\bar{m}-p} \in \mathcal{P}_{\bar{m}}$\cite{cg_sharpened_convrate_Axelsson1976}
            \end{itemize}
            }
        \end{column}
        \begin{column}{.5\textwidth}
            \only<2-4>{%
                \begin{figure}[H]
                    \centering
                    \resizebox{\textwidth}{!}{
                        \input{../tikz/eigenvalue_clusters.tex}
                    }
                \end{figure}
            }
            \only<5-8>{%
                Let $\epsilon = \frac{\|\mathbf{e}_m\|_A}{\|\mathbf{e}_0\|_A}$ relative error
                \begin{equation*}
                    1 \leq p = \min\left\{\left\lceil\frac{1}{2}\sqrt{\frac{b}{a}}\ln{\epsilon} + 1\right\rceil, N_{\text{tail}}\right\}
                \end{equation*}
            }
        \end{column}
    \end{columns}
    \only<6-8>{%
        \begin{equation*}
            \bar{m}=\left\lceil\frac{1}{2} \sqrt{\alert<7>{\frac{d}{c}}} \ln \frac{2}{\epsilon}+\left(1+\frac{1}{2} \sqrt{\alert<7>{\frac{d}{c}}} \ln \frac{4 \alert<8>{d}}{\alert<8>{e}}\right) p\right\rceil, \text{ where } e = \begin{cases}
                b, \text{ two clusters}\\
                a, \text{ cluster with tail}\\
            \end{cases}
        \end{equation*}
    }
\end{frame}

\footerinfootnotestrue
\begin{frame}[label=background,fragile]{Background: Schwarz}
    \frametitle{Background \& Literature}
    \framesubtitle{Schwarz preconditioners}
    \begin{columns}[T,onlytextwidth]
        \begin{column}{.5\textwidth}
            \begin{itemize}
                \item<1-> Derived from the Alternating Schwarz method\cite{schwarz_methods_Dolean_2015}
                \item<2-> Convergence rate depends on the overlap $\alert<2>{\delta}$ and the wave number of eigenmodes $\alert<3>{k}$
                \item<4-> As a preconditioner $M_{\text{ASM}} = \sum_{i=1}^{N_{\text{sub}}} R_i^T A_i^{-1} R_i$
                \item<5-> Need a coarse space $\alert<6>{R_0}$ to counter slowly converging modes
            \end{itemize}
        \end{column}
        \begin{column}{.49\textwidth}
            \only<1>{%
                \begin{figure}[H]
                    \input{../tikz/keyhole.tex}
                    \caption{Domain decomposition with overlapping subdomains.}
                \end{figure}
            }
            \only<2-3>{%
                \begin{exampleblock}{2D Alternating Schwarz Example}
                    Let $\Omega_1 = (-\infty, \delta)\times \mathbb{R}$, $\Omega_2 = (\delta, \infty)\times \mathbb{R}$
                    \begin{align*}
                        -(\eta - \Delta) u & = f \text{ in } \mathbb{R}^2, \\
                        u                  & \text{ bounded at infinity}.
                    \end{align*}
                    Then the convergence rate is given by
                    \begin{equation*}
                        \rho_{\text{2D}}(k;\eta,\delta) = e^{-\alert<2>{\delta}\sqrt{\eta + \alert<3>k^2}}
                    \end{equation*}
                \end{exampleblock} 
            }
            \only<4-5>{%
                \begin{figure}[H]
                    \centering
                    \input{../tikz/domain_decomposition.tex}
                    \caption{Domain decomposition with $N_{\text{sub}}$ subdomains.}
                \end{figure}
            }
            \only<6>{%
            \begin{block}{2-level Additive Schwarz Preconditioner}
                \begin{equation*}
                    M_{\text{ASM,2}} = \alert<6>{R_0}^T A_0^{-1} \alert<6>{R_0} + M_{\text{ASM}}
                \end{equation*}
            \end{block} 
            }
        \end{column}
    \end{columns}
\end{frame}

\footerinfootnotestrue
\begin{frame}[label=background, fragile]{Background: Coarse Spaces}
    \frametitle{Background \& Literature}
    \framesubtitle{Tailored Coarse Spaces for High-Contrast Problems}
    \only<4-6>{Figures obtained from \cite{ams_coarse_space_comp_study_Alves2024}} 
    \only<4-5>{\absimage{.73, .25}{.4}{method_legend_Alves.png}}
    \begin{columns}[T,onlytextwidth]
        \begin{column}{.5\textwidth}
            \only<1-3>{%
                \begin{itemize}
                    \item<1-> MsFEM-based: robust for high-contrast, but computationally expensive\footnote[frame]{\citeauthor{acms_coarse_space_Heinlein2018} (\citeyear{acms_coarse_space_Heinlein2018})}
                    \only<2-3>{
                        \item Generalized framework: GDSW\footnote[frame]{\citeauthor{gdsw_coarse_space_Dohrmann2008} (\citeyear{gdsw_coarse_space_Dohrmann2008})}, RGDSW\footnote[frame]{\citeauthor{rgdsw_coarse_space_Dohrmann2017} (\citeyear{rgdsw_coarse_space_Dohrmann2017})}
                    }
                        \only<3>{\item Hybrid: AMS\footnote[frame]{\citeauthor{ams_coarse_space_comp_study_Alves2024} (\citeyear{ams_coarse_space_comp_study_Alves2024})}
                    }
                \end{itemize}
            }
            \only<4>{%
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.8\textwidth]{coefficient_function_vertex_Alves.png}
                \end{figure}
            }
            \only<5>{%
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.8\textwidth]{coefficient_function_edges_Alves.png}
                \end{figure}
            }
        \end{column}
        \begin{column}{.5\textwidth}
            \only<2-3>{%
                \begin{figure}[H]
                    \centering
                    \input{../tikz/node_types.tex}
                \end{figure}
            }
            \only<4>{%
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=\textwidth]{cg_iterations_coefficient_vertex_Alves.png}
                \end{figure}
            }
            \only<5>{%
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=\textwidth]{cg_iterations_coefficient_edges_Alves.png}
                \end{figure}
            }
        \end{column}
    \end{columns}
    \only<6>{%
        \absimage{.5, .52}{.65}{precond_eigvals_8_and_64_subdomains_Alves.png}
    }
\end{frame}
